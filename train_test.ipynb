{
  "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Brownwang0426/Reversal-Generative-Reinforcement-Learning/blob/main/train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4khPQ2_Kf-v1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e66de59-6898-4817-b2b3-8444246f5fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m0y9RfWif-v2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e98eeaa-fe30-4886-fa11-b386cbadadc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==2.0.3\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.2.1\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting ufal.pybox2d==2.3.10.3\n",
            "  Downloading ufal.pybox2d-2.3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (416 bytes)\n",
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting pygame==2.5.2\n",
            "  Downloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ufal.pybox2d-2.3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ufal.pybox2d, swig, lit, pygame, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, scipy, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.1\n",
            "    Uninstalling pygame-2.6.1:\n",
            "      Successfully uninstalled pygame-2.6.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.14.3 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 numpy-1.25.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-2.0.3 pygame-2.5.2 scipy-1.11.4 swig-4.2.1 torch-2.0.1 triton-2.0.0 ufal.pybox2d-2.3.10.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "c02f5eff277a4dce8cb37c5232ca83d5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkdfX6yAPbPm"
      },
      "source": [
        "# Cloning git (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kVZs5loNPbPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7b8c1d-b9a8-49d3-f16a-066bef53acb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Reversal-Generative-Reinforcement-Learning'...\n",
            "remote: Enumerating objects: 3181, done.\u001b[K\n",
            "remote: Counting objects: 100% (783/783), done.\u001b[K\n",
            "remote: Compressing objects: 100% (290/290), done.\u001b[K\n",
            "remote: Total 3181 (delta 521), reused 706 (delta 489), pack-reused 2398 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3181/3181), 19.19 MiB | 10.42 MiB/s, done.\n",
            "Resolving deltas: 100% (2248/2248), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Brownwang0426/Reversal-Generative-Reinforcement-Learning.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2UlIqB3PbPo"
      },
      "source": [
        "# Changing directory (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hj-G_J3dPbPo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Reversal-Generative-Reinforcement-Learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Oa0SxAx5SFy",
        "outputId": "dcfaa225-16b4-453a-9d42-84e19046823b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Aj5V_vlwSxd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f505b0c4-28b6-4dfa-f7dc-976313d2f7b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device 0: Tesla T4\n",
            "using cuda...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "assert device != torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SlwYjPr7CYJd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial configurations regarding how your agent will learn in the environment. The meanings are as follow:\n",
        "(the configs starting with ⚠️ are what we suggest you must tune according to your specific need in your task)\n",
        "(the configs starting with ◀️ are what we suggest you to play with to see the effect)\n",
        "\n",
        "| Configs   | Type   | Description                                                                 |\n",
        "|------------|--------|-----------------------------------------------------------------------------|\n",
        "| ⚠️game_name  | STR| The name of the environment.                                |\n",
        "| ⚠️max_steps_for_each_episode | +INT | The maximun steps that the agent will go through while not done. In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.                    |\n",
        "| ◀️ensemble_size  | +INT | The size of the neural ensemble which the agent is comprised of. The bigger, the better, but the longer training time without parallel training. :-D                  |\n",
        "| ⚠️state_size  | +INT | The size of the state as input data.                    |\n",
        "| ⚠️hidden_size   | +INT |The size of the hidden layers. We suggest hidden_size >= state_size.           |\n",
        "| ⚠️action_size   | +INT | The size of action per step as input data.   |\n",
        "| ⚠️time_size  | +INT |The length of the sequence of actions. Namely, how many steps in the future the agent will predict or use to discern the present best action.                |\n",
        "| ⚠️reward_size  | +INT |The size of the reward as output data.                          |\n",
        "| ⚠️neural_type  | STR |  [**`rnn`**, **`gru`**, **`lstm`**, **`rnn_att`**] The type of neural network you prefer. For now, we support rnn, gru, lstm, and rnn_att (recurrent attention). More to come in the future (or you can build one yourself :-D in the models repository).           |\n",
        "| ⚠️num_layers  | +INT |The number of layers in rnn, gru, lstm, and rnn_att (recurrent attention). We suggest no less than 3 (>= 3) to provide more flexibility and memory capacity for neural networks.                         |\n",
        "| ⚠️num_heads  | +INT/None |The number of heads in multi-head attention (Should be able to devide hidden_size) (Should be None for non-attention neural_type).                         |\n",
        "| hidden_activation  | STR | [**`relu`**, **`leaky_relu`**, **`sigmoid`**, **`tanh`**] The type of activation function in the hidden layers.              |\n",
        "| output_activation  | STR | [**`relu`**, **`leaky_relu`**, **`sigmoid`**, **`tanh`**] The type of activation function in the output layer.                      |\n",
        "| shift  | 0/±FLOAT |The value in f(x+shift) where f(x) is activation function in the output layer. This value is interesting. If this value is negatively large, the agent will act more conservatively and prone to exploit known strategy. If this value is positively large, the agent to act more radically and prone to explore all possible strategies before settling down.      |\n",
        "| init   | STR | [**`random_normal`**, **`random_uniform`**, **`xavier_normal`**, **`xavier_uniform`**, **`glorot_normal`**, **`glorot_uniform`**] The initialization method you prefer.                          |\n",
        "| opti   | STR | [**`adam`**, **`sgd`**, **`rmsprop`**]  The optimization method you prefer.             |\n",
        "| loss  | STR | [**`mean_squared_error`**, **`binary_crossentropy`**] The loss or error function you prefer.                           |\n",
        "| bias  | BOLEAN |Whether you want add bias.                          |\n",
        "| drop_rate   | 0/+FLOAT |The drop-rate for drop-out.              |\n",
        "| ⚠️alpha   | 0/+FLOAT |The learning rate for neural networks weight matrices.                           |\n",
        "| ⚠️iteration_for_learning   | +INT |The iteration for learning.              |\n",
        "| load_pre_model  | BOLEAN |Whether you want to load previous trained model.                          |\n",
        "| noise_t  |  +INT |The times applying gaussian noise to the initializated actions of the agent, similar to diffusion model's adding gaussian noise.          |\n",
        "| ⚠️noise_r  |  0/+FLOAT |The noise range to the initializated actions of the agent. The higher the value is, the more exploration-oriented the agent will be.                    |\n",
        "| ⚠️noise_r_oscillation  |  +INT |The interval for which noise range will oscillate between noise_r and a very small number like 0.000001 to encourage agent to balance eploration and exploitation.                    |\n",
        "| ⚠️beta  |  0/+FLOAT |The updating rate for updating actions of the agent.              |\n",
        "| ⚠️iteration_for_deducing  |  +INT |The iteration for updating actions of the agent.                           |\n",
        "| episode_for_training  | +INT |How many epsiodes will your agent run in the training mode where your agent will learn offline.              |\n",
        "| chunk_size  | +INT |The maximum chunk size for sequentializing state, action, reward. We suggest chunk_size <= time_size.      |\n",
        "| batch_size_for_offline_learning  |+INT | After how many epsodes will your agent start learning from experience buffer.                           |\n",
        "| PER_epsilon  | 0/+FLOAT |The epsilon for prioritized experience replay.              |\n",
        "| PER_exponent  | 0/+FLOAT |The expoenet for prioritized experience replay.                           |\n",
        "| episode_for_testing  | +INT |How many epsiodes will your agent run in the testing mode where your agent will not learn offline.                        |\n",
        "| render_for_human  | BOLEAN | Wether you want to render the visual result for each step in the testing mode.              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ZJ8yAtPbPw"
      },
      "source": [
        "## frozen lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SMEvmlwvPbPw"
      },
      "outputs": [],
      "source": [
        "game_name =  'FrozenLake-v1'        #⚠️  gym.make(game_name, is_slippery=False, map_name=\"4x4\")\n",
        "max_steps_for_each_episode = 25     #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  16                    #⚠️\n",
        "hidden_size = 100                   #⚠️\n",
        "action_size = 4                     #⚠️\n",
        "time_size = 8                       #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 3                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "noise_t = 1                         #⚠️\n",
        "noise_r = 0.1                       #⚠️\n",
        "noise_r_oscillation = 10            #⚠️\n",
        "beta = 0.1                          #⚠️\n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size\n",
        "batch_size_for_offline_learning = 1\n",
        "PER_epsilon = 0.000001\n",
        "PER_exponent = 5\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASBb8wJPbPw"
      },
      "source": [
        "## blackjack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xH9VANzcPbPw"
      },
      "outputs": [],
      "source": [
        "game_name = 'Blackjack-v1'          #⚠️\n",
        "max_steps_for_each_episode = 10     #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  201                   #⚠️\n",
        "hidden_size = 250                   #⚠️\n",
        "action_size = 2                     #⚠️\n",
        "time_size = 5                       #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 3                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "noise_t = 1\n",
        "noise_r = 0.1                       #⚠️\n",
        "noise_r_oscillation = 10            #⚠️\n",
        "beta = 0.1                          #⚠️\n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size\n",
        "batch_size_for_offline_learning = 1\n",
        "PER_epsilon = 0.000001\n",
        "PER_exponent = 5\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgCIHGHWPbPx"
      },
      "source": [
        "## cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZlQB_D3UPbPx"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'           #⚠️\n",
        "max_steps_for_each_episode = 2000   #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 10                  #◀️\n",
        "state_size =  400                   #⚠️\n",
        "hidden_size = 400                   #⚠️\n",
        "action_size = 2                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 3                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "noise_t = 1                         #⚠️\n",
        "noise_r = 0.1                       #⚠️\n",
        "noise_r_oscillation = 10            #⚠️\n",
        "beta = 0.1                          #⚠️\n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size\n",
        "batch_size_for_offline_learning = 1\n",
        "PER_epsilon = 0.000001\n",
        "PER_exponent = 5\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MiLL2IAPbPx"
      },
      "source": [
        "## mountain car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m3uFeGuePbPx"
      },
      "outputs": [],
      "source": [
        "game_name =  'MountainCar-v0'       #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 10                  #◀️\n",
        "state_size =  200                   #⚠️\n",
        "hidden_size = 200                   #⚠️\n",
        "action_size = 3                     #⚠️\n",
        "time_size = 50                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 3                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "noise_t = 1\n",
        "noise_r = 0.1                       #⚠️\n",
        "noise_r_oscillation = 10            #⚠️\n",
        "beta = 0.1                          #⚠️\n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size\n",
        "batch_size_for_offline_learning = 1\n",
        "PER_epsilon = 0.000001\n",
        "PER_exponent = 5\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75C-eQ0QPbPy"
      },
      "source": [
        "## acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xQsfQiPsPbPy"
      },
      "outputs": [],
      "source": [
        "game_name = 'Acrobot-v1'            #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  600                   #⚠️\n",
        "hidden_size = 600                   #⚠️\n",
        "action_size = 3                     #⚠️\n",
        "time_size = 50                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 3                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "noise_t = 1\n",
        "noise_r = 0.1                       #⚠️\n",
        "noise_r_oscillation = 10            #⚠️\n",
        "beta = 0.1                          #⚠️\n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size\n",
        "batch_size_for_offline_learning = 1\n",
        "PER_epsilon = 0.000001\n",
        "PER_exponent = 5\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yi3S50oPbPy"
      },
      "source": [
        "## lunar lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_FUT8oRiPbPz"
      },
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v2\"        #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  800                   #⚠️\n",
        "hidden_size = 800                   #⚠️\n",
        "action_size = 4                     #⚠️\n",
        "time_size = 50                      #⚠️\n",
        "reward_size = 250                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 3                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "noise_t = 1                         #⚠️\n",
        "noise_r = 0.1                       #⚠️\n",
        "noise_r_oscillation = 10            #⚠️\n",
        "beta = 0.1                          #⚠️\n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size\n",
        "batch_size_for_offline_learning = 1\n",
        "PER_epsilon = 0.000001\n",
        "PER_exponent = 5\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wyup25fPbPz"
      },
      "source": [
        "## your present config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2qFfB3E5PbPz"
      },
      "outputs": [],
      "source": [
        "game_name =  'FrozenLake-v1'        #⚠️\n",
        "max_steps_for_each_episode = 25     #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  16                    #⚠️\n",
        "hidden_size = 100                   #⚠️\n",
        "action_size = 4                     #⚠️\n",
        "time_size = 8                       #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 3                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "noise_t = 1                         #⚠️\n",
        "noise_r = 0.1                       #⚠️\n",
        "noise_r_oscillation = 10            #⚠️\n",
        "beta = 0.1                          #⚠️\n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size\n",
        "batch_size_for_offline_learning = 1\n",
        "PER_epsilon = 0.000001\n",
        "PER_exponent = 5\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e2pcs2TbPbPz"
      },
      "outputs": [],
      "source": [
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={iteration_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={iteration_for_deducing:05d}\"\n",
        "directory                   = f'./result/{game_name}/'\n",
        "model_directory             = f'./result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'./result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TcwUHixPbPz"
      },
      "source": [
        "# Importing local modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rUk1dHbfPbPz"
      },
      "outputs": [],
      "source": [
        "if   game_name == 'FrozenLake-v1':\n",
        "    from envs.env_frozenlake   import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif   game_name == 'Blackjack-v1':\n",
        "    from envs.env_blackjack   import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif   game_name == 'CartPole-v1':\n",
        "    from envs.env_cartpole    import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'MountainCar-v0':\n",
        "    from envs.env_mountaincar import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'MountainCarContinuous-v0':\n",
        "    from envs.env_mountaincar_continuous import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'Acrobot-v1':\n",
        "    from envs.env_acrobot import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == \"Pendulum-v1\":\n",
        "    from envs.env_pendulum import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == \"LunarLander-v2\":\n",
        "    from envs.env_lunarlander import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'BipedalWalker-v3':\n",
        "    from envs.env_bipedalwalker import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "else:\n",
        "   raise RuntimeError('missing env functions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ErBxWJc2PbP0"
      },
      "outputs": [],
      "source": [
        "if neural_type == 'rnn_att':\n",
        "    from models.model_rnn_att import build_model\n",
        "    from utils.util_rnn_att   import initialize_pre_activated_actions, \\\n",
        "                                 update_pre_activated_actions, \\\n",
        "                                 sequentialize, \\\n",
        "                                 update_model,\\\n",
        "                                 save_performance_to_csv\n",
        "else:\n",
        "    from models.model_rnn import build_model\n",
        "    from utils.util_rnn_  import initialize_pre_activated_actions, \\\n",
        "                                 update_pre_activated_actions, \\\n",
        "                                 sequentialize, \\\n",
        "                                 update_model,\\\n",
        "                                 save_performance_to_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing -> Learning\n",
        "Training mode where your agent will learn offline. You can see here how your agent learn overtime and improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_list = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            shift,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            bias,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_list.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_list = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            shift,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            bias,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_list.append(model)\n",
        "\n",
        "    for i in range(len(model_list)):\n",
        "        model_list[i].load_state_dict(torch.load( model_directory  % i ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XdQIBpSxeF"
      },
      "source": [
        "Creating Streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Qfp24ueJSxeG"
      },
      "outputs": [],
      "source": [
        "stream_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    stream  = torch.cuda.Stream()\n",
        "    stream_list.append(stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6VzvFnSxeH"
      },
      "source": [
        "Creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_reward = torch.ones((1, time_size, reward_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2-jpi_m6p3RO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "outputId": "d9a75112-f341-44e3-f620-7fefae84a503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Summed_Reward = 0.0\n",
            "tensor([1, 1, 1, 2, 2, 3], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/100000 [02:07<3531:55:26, 127.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 123.0216 seconds\n",
            "Episode 1: Summed_Reward = 0.0\n",
            "tensor([1, 1, 1, 2, 2, 3, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3, 1], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/100000 [04:18<3597:09:38, 129.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 126.1485 seconds\n",
            "Episode 2: Summed_Reward = 0.0\n",
            "tensor([1, 1, 1, 2, 2, 3, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 2, 2, 3, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/100000 [06:33<3665:10:30, 131.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 131.6154 seconds\n",
            "Episode 3: Summed_Reward = 0.0\n",
            "tensor([1, 1, 1, 2, 2, 3, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/100000 [06:59<3882:23:44, 139.77s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2c864cc483e2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 model                     = update_model(iteration_for_learning,\n\u001b[0m\u001b[1;32m    129\u001b[0m                                                          \u001b[0mlong_term_present_state_tensors\u001b[0m  \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                                                          \u001b[0mlong_term_future_actions_tensors\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Reversal-Generative-Reinforcement-Learning/utils/util_rnn_.py\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(iteration_for_learning, long_term_present_state_tensors, long_term_future_actions_tensors, long_term_future_rewards_tensors, long_term_future_states_tensors, long_term_pad_size_tensors, model, PER_epsilon, PER_exponent, device)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0moutput_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_state_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_actions_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mtotal_loss\u001b[0m                  \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_rewards_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpad_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_states_tensor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpad_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# get grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mselected_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "performance_log  = []\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    list_states  = []\n",
        "    list_actions = []\n",
        "    list_rewards = []\n",
        "\n",
        "    # initializing environment\n",
        "    env                    = gym.make(game_name, is_slippery=False, map_name=\"4x4\")\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    summed_reward          = 0\n",
        "\n",
        "    # observing state\n",
        "    state    = vectorizing_state(state)\n",
        "    list_states.append(state)\n",
        "\n",
        "    for count in itertools.count(1):\n",
        "\n",
        "        print(f'\\rStep: {count}\\r', end='', flush=True)\n",
        "\n",
        "        # initializing and updating action\n",
        "        state                 = torch.tensor(np.atleast_2d(state), dtype=torch.float)\n",
        "        t_oscillation         =  (noise_t if (training_episode % (2 * noise_r_oscillation) < noise_r_oscillation) else 1)\n",
        "        r_oscillation         =  (noise_r if (training_episode % (2 * noise_r_oscillation) < noise_r_oscillation) else 0.000001)\n",
        "        pre_activated_action  = initialize_pre_activated_actions(init,\n",
        "                                                                t_oscillation,\n",
        "                                                                r_oscillation,\n",
        "                                                                (time_size, action_size))\n",
        "        pre_activated_action  = torch.tensor(pre_activated_action[np.newaxis, :, :], dtype=torch.float)\n",
        "        pre_activated_action  = update_pre_activated_actions(iteration_for_deducing,\n",
        "                                                             model_list,\n",
        "                                                             state,\n",
        "                                                             pre_activated_action,\n",
        "                                                             desired_reward,\n",
        "                                                             beta,\n",
        "                                                             device)\n",
        "        action, action_       = vectorizing_action(pre_activated_action)\n",
        "        list_actions.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, info = env.step(action_)\n",
        "\n",
        "        # observing actual reward\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, done, reward_size)\n",
        "        list_rewards.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state    = vectorizing_state(state)\n",
        "        list_states.append(state)\n",
        "\n",
        "        if done:\n",
        "            print(f'Episode {training_episode}: Summed_Reward = {summed_reward}')\n",
        "            performance_log.append([training_episode, summed_reward])\n",
        "            save_performance_to_csv(performance_log, performance_log_directory)\n",
        "            break\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentializing short term experience replay buffer\n",
        "    present_state_tensors   ,\\\n",
        "    future_actions_tensors  ,\\\n",
        "    future_rewards_tensors   ,\\\n",
        "    future_states_tensors    ,\\\n",
        "    pad_size_tensors            = sequentialize(list_states  ,\n",
        "                                            list_actions ,\n",
        "                                            list_rewards , chunk_size, device)\n",
        "\n",
        "    if training_episode == 0:\n",
        "      long_term_present_state_tensors   = copy.deepcopy(present_state_tensors  )\n",
        "      long_term_future_actions_tensors  = copy.deepcopy(future_actions_tensors )\n",
        "      long_term_future_rewards_tensors   = copy.deepcopy(future_rewards_tensors  )\n",
        "      long_term_future_states_tensors    = copy.deepcopy(future_states_tensors   )\n",
        "      long_term_pad_size_tensors            = copy.deepcopy(pad_size_tensors           )\n",
        "\n",
        "      # storing sequentialized short term experience to long term experience replay buffer by length when it is new\n",
        "      existing_hashes = set(\n",
        "          hash((tuple(t.numpy().flatten()) for t in tensors))\n",
        "          for tensors in zip(\n",
        "              long_term_present_state_tensors,\n",
        "              long_term_future_actions_tensors,\n",
        "              long_term_future_rewards_tensors,\n",
        "              long_term_future_states_tensors,\n",
        "              long_term_pad_size_tensors,\n",
        "          )\n",
        "      )\n",
        "    else:\n",
        "      for i in range(len(present_state_tensors)):\n",
        "          new_sample = (\n",
        "              present_state_tensors[i],\n",
        "              future_actions_tensors[i],\n",
        "              future_rewards_tensors[i],\n",
        "              future_states_tensors[i],\n",
        "              pad_size_tensors[i]\n",
        "          )\n",
        "          sample_hash = hash(tuple(t.numpy().flatten()) for t in new_sample)\n",
        "          if sample_hash not in existing_hashes:\n",
        "\n",
        "              long_term_present_state_tensors  = torch.cat((long_term_present_state_tensors  , new_sample[0].unsqueeze(0)), dim=0)\n",
        "              long_term_future_actions_tensors = torch.cat((long_term_future_actions_tensors , new_sample[1].unsqueeze(0)), dim=0)\n",
        "              long_term_future_rewards_tensors  = torch.cat((long_term_future_rewards_tensors  , new_sample[2].unsqueeze(0)), dim=0)\n",
        "              long_term_future_states_tensors   = torch.cat((long_term_future_states_tensors   , new_sample[3].unsqueeze(0)), dim=0)\n",
        "              long_term_pad_size_tensors           = torch.cat((long_term_pad_size_tensors           , new_sample[4].unsqueeze(0)), dim=0)\n",
        "\n",
        "              existing_hashes.add(sample_hash)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # batch offline learning\n",
        "    if (training_episode+1) % batch_size_for_offline_learning == 0:\n",
        "\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "\n",
        "        # training with Prioritized Experience Replay (PER)\n",
        "        for i, model in enumerate(model_list):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "                model                     = update_model(iteration_for_learning,\n",
        "                                                         long_term_present_state_tensors  ,\n",
        "                                                         long_term_future_actions_tensors ,\n",
        "                                                         long_term_future_rewards_tensors  ,\n",
        "                                                         long_term_future_states_tensors   ,\n",
        "                                                         long_term_pad_size_tensors           ,\n",
        "                                                         model,\n",
        "                                                         PER_epsilon,\n",
        "                                                         PER_exponent,\n",
        "                                                         device)\n",
        "                model_list[i]             = model\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "        end_time = time.time()  # Record end time\n",
        "        execution_time = end_time - start_time  # Calculate duration\n",
        "        print(f\"Execution Time: {execution_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "        # saving:\n",
        "        for i in range(len(model_list)):\n",
        "            torch.save(model_list[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing only\n",
        "Testing mode where your trained agent in the training mode will not learn offline. It just keeps running each episode without learning new stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        hidden_size,\n",
        "                        action_size,\n",
        "                        time_size,\n",
        "                        reward_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        shift,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        bias,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "for i in range(len(model_list)):\n",
        "    model_list[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRIFvTYSxeJ"
      },
      "source": [
        "Creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_reward = torch.ones((1, time_size, reward_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "total_summed_reward = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, render_mode=\"human\")\n",
        "    else:\n",
        "        env = gym.make( game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = vectorizing_state(state)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        state                 = torch.tensor(np.atleast_2d(state), dtype=torch.float)\n",
        "        pre_activated_action  = initialize_pre_activated_actions(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_action  = torch.tensor(pre_activated_action[np.newaxis, :, :], dtype=torch.float)\n",
        "        pre_activated_action  = update_pre_activated_actions(iteration_for_deducing,\n",
        "                                                             model_list,\n",
        "                                                             state,\n",
        "                                                             pre_activated_action,\n",
        "                                                             desired_reward,\n",
        "                                                             beta,\n",
        "                                                             device)\n",
        "        action, action_       = vectorizing_action(pre_activated_action)\n",
        "\n",
        "        state, reward, done,  info = env.step(action_)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "\n",
        "        summed_reward += reward\n",
        "\n",
        "        state = vectorizing_state(state)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDqzK4Ht5SF5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Input tensor\n",
        "input_tensor = torch.tensor([4, 2, 3])  # Shape: (3,)\n",
        "\n",
        "# Fixed length for each row\n",
        "fixed_length = 50\n",
        "\n",
        "# Generate a range for the fixed length\n",
        "range_tensor = torch.arange(fixed_length)  # Shape: (fixed_length,)\n",
        "print(range_tensor)\n",
        "# Compare and create the mask\n",
        "output_tensor = (range_tensor < input_tensor.unsqueeze(1)).int()  # Shape: (3, fixed_length)\n",
        "\n",
        "print(output_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example tensors\n",
        "tensor_3d = torch.randn(2, 3, 999)  # Shape: (batch_size, num_rows, feature_size)\n",
        "print(tensor_3d)\n",
        "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (batch_size, num_rows)\n",
        "print(tensor_2d)\n",
        "# Expand the 2D tensor to match the 3D tensor's shape for broadcasting\n",
        "tensor_2d_expanded = tensor_2d.unsqueeze(-1)  # Shape: (batch_size, num_rows, 1)\n",
        "\n",
        "# Perform element-wise multiplication\n",
        "result = tensor_3d * tensor_2d_expanded\n",
        "print(result)\n",
        "print(f\"3D Tensor Shape: {tensor_3d.shape}\")\n",
        "print(f\"2D Tensor Shape: {tensor_2d.shape}\")\n",
        "print(f\"Expanded 2D Tensor Shape: {tensor_2d_expanded.shape}\")\n",
        "print(f\"Result Shape: {result.shape}\")\n"
      ],
      "metadata": {
        "id": "wywg2VIgfzX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-bRZFaVhxXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
