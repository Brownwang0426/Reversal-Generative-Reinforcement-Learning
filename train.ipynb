{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/Reversal-Generative-Reinforcement-Learning/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4khPQ2_Kf-v1"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0y9RfWif-v2"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkdfX6yAPbPm"
      },
      "source": [
        "# Cloning git (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVZs5loNPbPn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Brownwang0426/Reversal-Generative-Reinforcement-Learning.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2UlIqB3PbPo"
      },
      "source": [
        "# Changing directory (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj-G_J3dPbPo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Reversal-Generative-Reinforcement-Learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJJ6Gty98unc"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj5V_vlwSxd8"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlwYjPr7CYJd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial configurations regarding how your agent will learn in the environment. The meanings are as follow:\n",
        "(the configs starting with ⚠️ are what we suggest you must tune according to your specific need in your task)\n",
        "(the configs starting with ◀️ are what we suggest you to play with to see the effect)\n",
        "\n",
        "| Configs   | Type   | Description                                                                 |\n",
        "|------------|--------|-----------------------------------------------------------------------------|\n",
        "| ⚠️game_name  | STR| The name of the environment.                                |\n",
        "| ⚠️max_steps_for_each_episode | +INT | The maximun steps that the agent will go through while not done. In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.                    |\n",
        "| ◀️ensemble_size  | +INT | The size of the neural ensemble which the agent is comprised of. The bigger, the better, but the longer training time without parallel training. :-D                  |\n",
        "| ⚠️state_size  | +INT | The size of the state as input data.                    |\n",
        "| ⚠️hidden_size   | +INT |The size of the hidden layers. We suggest hidden_size >= state_size.           |\n",
        "| ⚠️action_size   | +INT | The size of action per step as input data.   |\n",
        "| ⚠️time_size  | +INT |The length of the sequence of actions. Namely, how many steps in the future the agent will predict or use to discern the present best action.                |\n",
        "| ⚠️reward_size  | +INT |The size of the reward as output data.                          |\n",
        "| ⚠️neural_type  | STR |  [**`rnn`**, **`gru`**, **`lstm`**, **`rnn_att`**] The type of neural network you prefer. For now, we support rnn, gru, lstm, and rnn_att (recurrent attention). More to come in the future (or you can build one yourself :-D in the models repository).           |\n",
        "| ⚠️num_layers  | +INT |The number of layers in rnn, gru, lstm, and rnn_att (recurrent attention). We suggest no less than 2 (>= 2) to provide more flexibility and memory capacity for neural networks.                         |\n",
        "| ⚠️num_heads  | +INT/None |The number of heads in multi-head attention (Should be able to devide hidden_size) (Should be None for non-attention neural_type).                         |\n",
        "| hidden_activation  | STR | [**`relu`**, **`leaky_relu`**, **`sigmoid`**, **`tanh`**] The type of activation function in the hidden layers.              |\n",
        "| output_activation  | STR | [**`relu`**, **`leaky_relu`**, **`sigmoid`**, **`tanh`**] The type of activation function in the output layer.                      |\n",
        "| shift  | 0/±FLOAT |The value in f(x+shift) where f(x) is activation function in the output layer. This value is interesting. If this value is negatively large, the agent will act more conservatively and prone to exploit known strategy. If this value is positively large, the agent to act more radically and prone to explore all possible strategies before settling down. We humorously refer to this variable as the \"playboy variable,\" drawing an analogy to individuals who change partners frequently in search of the ideal match because the individuals always think there might be better choice :-P But we can't really write this into the paper... you know...             |\n",
        "| init   | STR | [**`random_normal`**, **`random_uniform`**, **`xavier_normal`**, **`xavier_uniform`**, **`glorot_normal`**, **`glorot_uniform`**] The initialization method you prefer.                          |\n",
        "| opti   | STR | [**`adam`**, **`sgd`**, **`rmsprop`**]  The optimization method you prefer.             |\n",
        "| loss  | STR | [**`mean_squared_error`**, **`binary_crossentropy`**] The loss or error function you prefer.                           |\n",
        "| bias  | BOLEAN |Whether you want add bias.                          |\n",
        "| drop_rate   | 0/+FLOAT |The drop-rate for drop-out.              |\n",
        "| ⚠️alpha   | 0/+FLOAT |The learning rate for neural networks weight matrices.                           |\n",
        "| ⚠️iteration_for_learning   | +INT |The iteration for learning.              |\n",
        "| load_pre_model  | BOLEAN |Whether you want to load previous trained model.                          |\n",
        "| greed_epsilon_t  |  +INT |The times applying gaussian noise to the initializated actions of the agent, similar to diffusion model's adding gaussian noise.          |\n",
        "| greed_epsilon_init  |  +FLOAT |The initial greed_epsilon or noise range to initializate the actions of the agent. The higher the value is, the more exploration-oriented the agent will be in the begining.                    |\n",
        "| greed_epsilon_min  |  +FLOAT |A very small number representing the lower bound of the greed_epsilon.        |\n",
        "| greed_epsilon_decay  |  +FLOAT | The rate of decaying for greed_epsilon for each step and eposide.  |\n",
        "| episode_for_rebound  | +INT |How many epsiodes will your agent use e-greedy to explore the environement before it fully exploits known expereince. In nutshell, we use **`epsilon annealing with a rebound`**.              |\n",
        "| beta  |  0/+FLOAT |The updating rate for updating actions of the agent.              |\n",
        "| iteration_for_deducing  |  +INT |The iteration for updating actions of the agent.                           |\n",
        "| episode_for_training  | +INT |How many epsiodes will your agent run in the training mode where your agent will learn offline.              |\n",
        "| chunk_size  | +INT |The maximum chunk size for sequentializing state, action, reward. We suggest chunk_size <= time_size.      |\n",
        "| batch_size_for_offline_learning  |+INT | After how many epsodes will your agent start learning from experience buffer.                           |\n",
        "| PER_epsilon  | 0/+FLOAT |The epsilon for prioritized experience replay.              |\n",
        "| PER_exponent  | 0/+FLOAT |The expoenet for prioritized experience replay.                           |\n",
        "| episode_for_testing  | +INT |How many epsiodes will your agent run in the testing mode where your agent will not learn offline.                        |\n",
        "| render_for_human  | BOLEAN | Wether you want to render the visual result for each step in the testing mode.              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ZJ8yAtPbPw"
      },
      "source": [
        "## frozen lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMEvmlwvPbPw"
      },
      "outputs": [],
      "source": [
        "game_name =  'FrozenLake-v1'        #⚠️   gym.make(game_name, is_slippery=False, map_name=\"4x4\")\n",
        "max_steps_for_each_episode = 25     #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  16                    #⚠️\n",
        "hidden_size = 100                   #⚠️\n",
        "action_size = 4                     #⚠️\n",
        "time_size = 8                       #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_uniform\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASBb8wJPbPw"
      },
      "source": [
        "## blackjack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH9VANzcPbPw"
      },
      "outputs": [],
      "source": [
        "game_name = 'Blackjack-v1'          #⚠️\n",
        "max_steps_for_each_episode = 10     #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  201                   #⚠️\n",
        "hidden_size = 250                   #⚠️\n",
        "action_size = 2                     #⚠️\n",
        "time_size = 5                       #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_uniform\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgCIHGHWPbPx"
      },
      "source": [
        "## cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlQB_D3UPbPx"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'           #⚠️\n",
        "max_steps_for_each_episode = 2000   #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  400                   #⚠️\n",
        "hidden_size = 400                   #⚠️\n",
        "action_size = 2                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_uniform\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MiLL2IAPbPx"
      },
      "source": [
        "## mountain car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3uFeGuePbPx"
      },
      "outputs": [],
      "source": [
        "game_name =  'MountainCar-v0'       #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  200                   #⚠️\n",
        "hidden_size = 200                   #⚠️\n",
        "action_size = 3                     #⚠️\n",
        "time_size = 50                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_uniform\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75C-eQ0QPbPy"
      },
      "source": [
        "## acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQsfQiPsPbPy"
      },
      "outputs": [],
      "source": [
        "game_name = 'Acrobot-v1'            #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  600                   #⚠️\n",
        "hidden_size = 600                   #⚠️\n",
        "action_size = 3                     #⚠️\n",
        "time_size = 50                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_uniform\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yi3S50oPbPy"
      },
      "source": [
        "## lunar lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FUT8oRiPbPz"
      },
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v2\"        #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  800                   #⚠️\n",
        "hidden_size = 800                   #⚠️\n",
        "action_size = 4                     #⚠️\n",
        "time_size = 50                      #⚠️\n",
        "reward_size = 250                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_uniform\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wyup25fPbPz"
      },
      "source": [
        "## your present config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qFfB3E5PbPz"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'           #⚠️\n",
        "max_steps_for_each_episode = 2000   #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  400                   #⚠️\n",
        "hidden_size = 400                   #⚠️\n",
        "action_size = 2                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "shift = 0.0\n",
        "init = \"random_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️\n",
        "iteration_for_learning = 1000       #⚠️\n",
        "load_pre_model = False\n",
        "\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 2.718\n",
        "greed_epsilon_min   = 0.000001\n",
        "greed_epsilon_decay = 0.95\n",
        "episode_for_rebound = 100\n",
        "beta = 0.1\n",
        "iteration_for_deducing = 100\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size\n",
        "\n",
        "\n",
        "batch_size_for_offline_learning = 1\n",
        "PER_epsilon  = 0.000001\n",
        "PER_exponent = 2\n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pcs2TbPbPz"
      },
      "outputs": [],
      "source": [
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={iteration_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={iteration_for_deducing:05d}\"\n",
        "directory                   = f'./result/{game_name}/'\n",
        "model_directory             = f'./result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'./result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TcwUHixPbPz"
      },
      "source": [
        "# Importing local modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUk1dHbfPbPz"
      },
      "outputs": [],
      "source": [
        "if   game_name == 'FrozenLake-v1':\n",
        "    from envs.env_frozenlake   import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif   game_name == 'Blackjack-v1':\n",
        "    from envs.env_blackjack   import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif   game_name == 'CartPole-v1':\n",
        "    from envs.env_cartpole    import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'MountainCar-v0':\n",
        "    from envs.env_mountaincar import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'MountainCarContinuous-v0':\n",
        "    from envs.env_mountaincar_continuous import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'Acrobot-v1':\n",
        "    from envs.env_acrobot import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == \"Pendulum-v1\":\n",
        "    from envs.env_pendulum import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == \"LunarLander-v2\":\n",
        "    from envs.env_lunarlander import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'BipedalWalker-v3':\n",
        "    from envs.env_bipedalwalker import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "else:\n",
        "   raise RuntimeError('missing env functions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErBxWJc2PbP0"
      },
      "outputs": [],
      "source": [
        "if neural_type == 'rnn_att':\n",
        "    from models.model_rnn_att import build_model\n",
        "    from utils.util_rnn_att   import initialize_pre_activated_action, \\\n",
        "                                 update_pre_activated_action, \\\n",
        "                                 sequentialize, \\\n",
        "                                 update_long_term_experience_buffer,\\\n",
        "                                 update_model_parallel,\\\n",
        "                                 save_performance_to_csv\n",
        "else:\n",
        "    from models.model_rnn import build_model\n",
        "    from utils.util_rnn   import initialize_pre_activated_action, \\\n",
        "                                 update_pre_activated_action, \\\n",
        "                                 sequentialize, \\\n",
        "                                 update_long_term_experience_buffer,\\\n",
        "                                 update_model_parallel,\\\n",
        "                                 save_performance_to_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing -> Learning\n",
        "Training mode where your agent will learn offline. You can see here how your agent learn overtime and improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_list = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            shift,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            bias,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_list.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_list = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            shift,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            bias,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_list.append(model)\n",
        "\n",
        "    for i in range(len(model_list)):\n",
        "        model_list[i].load_state_dict(torch.load( model_directory  % i ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "16babb4f-2ae8-4018-f321-f8e64fd7c5b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "# creating desired reward\n",
        "# - We dont randomize desired reward anymore because it is not typical in RL \\\n",
        "# - and there are many more effective methods like epsilon-greedy, intrinsic motivation, and reward shaping \\\n",
        "# - that can drive an agent to explore effectively. These methods are designed to balance exploration and exploitation in a way \\\n",
        "# - that promotes learning while keeping the agent on a meaningful path toward mastering the environment.\n",
        "desired_reward = torch.ones ((1, time_size, reward_size))\n",
        "\n",
        "# creating empty log for recording performance\n",
        "performance_log  = []\n",
        "\n",
        "# creating empty dictionary for storing tensors\n",
        "present_state_tensor_dict = defaultdict(lambda: torch.Tensor().to(device))\n",
        "future_action_tensor_dict = defaultdict(lambda: torch.Tensor().to(device))\n",
        "future_reward_tensor_dict = defaultdict(lambda: torch.Tensor().to(device))\n",
        "future_state_tensor_dict  = defaultdict(lambda: torch.Tensor().to(device))\n",
        "present_state_hash_list   = list()\n",
        "future_action_hash_list   = list()\n",
        "future_reward_hash_list   = list()\n",
        "future_state_hash_list    = list()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# starting each episode\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    # initialize e-greedy\n",
        "    greed_epsilon = copy.deepcopy(greed_epsilon_init)\n",
        "\n",
        "    # initialize e-greedy decay rate\n",
        "    decay         = greed_epsilon_decay ** (training_episode % episode_for_rebound)\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    state_list  = []\n",
        "    action_list = []\n",
        "    reward_list = []\n",
        "\n",
        "    # initializing environment\n",
        "    env                    = gym.make(game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    summed_reward          = 0\n",
        "\n",
        "    # observing state\n",
        "    state    = vectorizing_state(state, device)\n",
        "    state_list.append(state)\n",
        "\n",
        "    # starting each step\n",
        "    for count in itertools.count(1):\n",
        "        print(f'\\rStep: {count}\\r', end='', flush=True)\n",
        "\n",
        "        # using e-greedy to randomize action for agent to fully explore the environment and escape local minima\n",
        "        # greed_epsilon drops for each step and episode\n",
        "        if (training_episode % (2 * episode_for_rebound)) < episode_for_rebound:\n",
        "            greed_epsilon = max( greed_epsilon_min, greed_epsilon * decay )\n",
        "        else:\n",
        "            greed_epsilon = copy.deepcopy(greed_epsilon_min)\n",
        "\n",
        "        # initializing and updating action\n",
        "        pre_activated_action  = initialize_pre_activated_action(init_, greed_epsilon_t, greed_epsilon, (time_size, action_size), device)\n",
        "        pre_activated_action  = update_pre_activated_action(iteration_for_deducing,\n",
        "                                                            model_list,\n",
        "                                                            state.unsqueeze(0),\n",
        "                                                            pre_activated_action.unsqueeze(0),\n",
        "                                                            desired_reward,\n",
        "                                                            beta,\n",
        "                                                            device)\n",
        "        action, action_       = vectorizing_action(pre_activated_action, device)\n",
        "        action_list.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, info = env.step(action_)\n",
        "\n",
        "        # observing actual reward\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, done, reward_size, device)\n",
        "        reward_list.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state    = vectorizing_state(state, device)\n",
        "        state_list.append(state)\n",
        "\n",
        "        # saving final reward to log if done\n",
        "        if done:\n",
        "            print('-------------------------------------------')\n",
        "            print(f'Episode {training_episode}: Summed_Reward = {summed_reward}')\n",
        "            print('-------------------------------------------')\n",
        "            performance_log.append([training_episode, summed_reward])\n",
        "            save_performance_to_csv(performance_log, performance_log_directory)\n",
        "            break\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # closing env\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentializing short term experience replay buffer\n",
        "    start_time = time.time()\n",
        "    present_state_list   ,\\\n",
        "    future_action_list   ,\\\n",
        "    future_reward_list   ,\\\n",
        "    future_state_list    = sequentialize(state_list  ,\n",
        "                                         action_list ,\n",
        "                                         reward_list , chunk_size, device)\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution Time for preparing epxperience buffer: {execution_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # storing sequentialized short term experience to long term experience replay buffer by length when it is a new experience\n",
        "    # these lines of code do not seem pretty though...\n",
        "    start_time = time.time()\n",
        "    present_state_tensor_dict, \\\n",
        "    future_action_tensor_dict, \\\n",
        "    future_reward_tensor_dict, \\\n",
        "    future_state_tensor_dict , \\\n",
        "    present_state_hash_list  , \\\n",
        "    future_action_hash_list  , \\\n",
        "    future_reward_hash_list  , \\\n",
        "    future_state_hash_list   = update_long_term_experience_buffer(present_state_tensor_dict,\n",
        "                                                                  future_action_tensor_dict,\n",
        "                                                                  future_reward_tensor_dict,\n",
        "                                                                  future_state_tensor_dict ,\n",
        "                                                                  present_state_hash_list  ,\n",
        "                                                                  future_action_hash_list  ,\n",
        "                                                                  future_reward_hash_list  ,\n",
        "                                                                  future_state_hash_list   ,\n",
        "                                                                  present_state_list,\n",
        "                                                                  future_action_list,\n",
        "                                                                  future_reward_list,\n",
        "                                                                  future_state_list )\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution Time for storing epxperience buffer  : {execution_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # batch offline learning\n",
        "    if (training_episode+1) % batch_size_for_offline_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # training with Prioritized Experience Replay (PER)\n",
        "        start_time = time.time()\n",
        "        model_list = update_model_parallel(\n",
        "                                           iteration_for_learning ,\n",
        "                                           present_state_tensor_dict,\n",
        "                                           future_action_tensor_dict,\n",
        "                                           future_reward_tensor_dict,\n",
        "                                           future_state_tensor_dict ,\n",
        "                                           model_list,\n",
        "                                           PER_epsilon,\n",
        "                                           PER_exponent,\n",
        "                                           device\n",
        "                                           )\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        print(f\"Execution Time for training neural net ensemble: {execution_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # saving:\n",
        "        for i in range(len(model_list)):\n",
        "            torch.save(model_list[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # clear up\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing only\n",
        "Testing mode where your trained agent in the training mode will not learn offline. It just keeps running each episode without learning new stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        hidden_size,\n",
        "                        action_size,\n",
        "                        time_size,\n",
        "                        reward_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        shift,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        bias,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "for i in range(len(model_list)):\n",
        "    model_list[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "desired_reward = torch.ones((1, time_size, reward_size))\n",
        "\n",
        "total_summed_reward = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    greed_epsilon = copy.deepcopy(greed_epsilon_min)\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, render_mode=\"human\")\n",
        "    else:\n",
        "        env = gym.make( game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = vectorizing_state(state, device)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        pre_activated_action  = initialize_pre_activated_action(init_, greed_epsilon_t, greed_epsilon, (time_size, action_size), device)\n",
        "        pre_activated_action  = update_pre_activated_action(iteration_for_deducing,\n",
        "                                                            model_list,\n",
        "                                                            state.unsqueeze(0),\n",
        "                                                            pre_activated_action.unsqueeze(0),\n",
        "                                                            desired_reward,\n",
        "                                                            beta,\n",
        "                                                            device)\n",
        "        action, action_       = vectorizing_action(pre_activated_action, device)\n",
        "\n",
        "        state, reward, done,  info = env.step(action_)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "\n",
        "        summed_reward += reward\n",
        "\n",
        "        state = vectorizing_state(state, device)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIAqtsLn8unm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}