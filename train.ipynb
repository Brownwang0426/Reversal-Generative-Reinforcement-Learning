{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/RGRL/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cloning git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !git clone https://Brownwang0426:token@github.com/Brownwang0426/RGRL.git\n",
        "!git clone https://github.com/Brownwang0426/RGRL.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khPQ2_Kf-v1",
        "outputId": "3350c060-7302-4eeb-bfc9-f73f03974e0e"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0y9RfWif-v2",
        "outputId": "31b7bb14-f74e-4aaf-fe69-4cebbdb8bbcf"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==2.0.3 numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gym==0.25.2 pygame==2.5.2 tqdm torch==2.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Changing directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/RGRL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5V_vlwSxd8",
        "outputId": "af231b3d-b797-492f-b1ad-2d5ce1b94ff0"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "assert device != torch.device(\"cpu\") # Sorry, but we really recommend you to run it on GPU :-) Nvidia needs your money :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SlwYjPr7CYJd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial configurations regarding how your agent will learn in the environment. The meanings are as follow:\n",
        "(the configs starting with ⚠️ are what we suggest you must tune according to your specific need in your task)\n",
        "(the configs starting with ◀️ are what we suggest you to play with to see the effect)\n",
        "\n",
        "| Configs   | Type   | Description                                                                 |\n",
        "|------------|--------|-----------------------------------------------------------------------------|\n",
        "| ⚠️game_name  | STR| The name of the environment.                                |\n",
        "| ⚠️max_steps_for_each_episode | +INT | The maximun steps that the agent will go through while not done. In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.                    |\n",
        "| ◀️ensemble_size  | +INT | The size of the neural ensemble which the agent is comprised of. The bigger, the better, but the longer training time without parallel training. :-D                  |\n",
        "| ⚠️state_size  | +INT | The size of the state as input data.                    |\n",
        "| ⚠️hidden_size   | +INT |The size of the hidden layers. We suggest hidden_size >= state_size.           |\n",
        "| ⚠️action_size   | +INT | The size of action per step as input data.   |\n",
        "| ⚠️time_size  | +INT |The length of the sequence of actions. Namely, how many steps in the future the agent will predict or use to discern the present best action.                |\n",
        "| ⚠️reward_size  | +INT |The size of the reward as output data.                          |\n",
        "| ⚠️neural_type  | STR |  [**`rnn`**, **`gru`**, **`lstm`**, **`rnn_att`**] The type of neural network you prefer. For now, we support rnn, gru, lstm, and rnn_att (recurrent attention). More to come in the future (or you can build one yourself :-D in the models repository).           |\n",
        "| ⚠️num_layers  | +INT |The number of layers in rnn, gru, lstm, and rnn_att (recurrent attention). We suggest no less than 2 (>= 2) to provide more flexibility and memory capacity for neural networks.                         |\n",
        "| ⚠️num_heads  | +INT/None |The number of heads in multi-head attention (Should be able to devide hidden_size) (Should be None for non-attention neural_type).                         |\n",
        "| hidden_activation  | STR | [**`relu`**, **`leaky_relu`**, **`sigmoid`**, **`tanh`**] The type of activation function in the hidden layers.              |\n",
        "| output_activation  | STR | [**`relu`**, **`leaky_relu`**, **`sigmoid`**, **`tanh`**] The type of activation function in the output layer.                      |\n",
        "| shift  | 0/±FLOAT |The value in f(x+shift) where f(x) is activation function in the output layer. This value is interesting. If this value is negatively large, the agent will act more conservatively and prone to exploit known strategy. If this value is positively large, the agent to act more radically and prone to explore all possible strategies before settling down. We humorously refer to this variable as the \"playboy variable,\" drawing an analogy to individuals who change partners frequently in search of the ideal match because the individuals always think there might be better choice :-P But we can't really write this into the paper... you know...             |\n",
        "| init   | STR | [**`random_normal`**, **`random_uniform`**, **`xavier_normal`**, **`xavier_uniform`**, **`glorot_normal`**, **`glorot_uniform`**] The initialization method you prefer.                          |\n",
        "| opti   | STR | [**`adam`**, **`sgd`**, **`rmsprop`**]  The optimization method you prefer.             |\n",
        "| loss  | STR | [**`mean_squared_error`**, **`binary_crossentropy`**] The loss or error function you prefer.                           |\n",
        "| bias  | BOLEAN |Whether you want add bias.                          |\n",
        "| drop_rate   | 0/+FLOAT |The drop-rate for drop-out.              |\n",
        "| ⚠️alpha   | 0/+FLOAT |The learning rate for neural networks weight matrices.                           |\n",
        "| ⚠️iteration_for_learning   | +INT |The iteration for learning.              |\n",
        "| load_pre_model  | BOLEAN |Whether you want to load previous trained model.                          |\n",
        "| noise_t  |  +INT |The times applying gaussian noise to the initializated actions of the agent, similar to diffusion model's adding gaussian noise.          |\n",
        "| noise_r  |  0/+FLOAT |The noise range to the initializated actions of the agent.                     |\n",
        "| ⚠️beta  |  0/+FLOAT |The updating rate for updating actions of the agent.              |\n",
        "| ⚠️iteration_for_deducing  |  +INT |The iteration for updating actions of the agent.                           |\n",
        "| episode_for_training  | +INT |How many epsiodes will your agent run in the training mode where your agent will learn offline.              |\n",
        "| chunk_size  | +INT |The maximum chunk size for sequentializing state, action, reward. We suggest chunk_size = time_size.      |\n",
        "| batch_size_for_offline_learning  |+INT | After how many epsodes will your agent start learning from experience buffer.                           |\n",
        "| PER_epsilon  | 0/+FLOAT |The epsilon for prioritized experience replay.              |\n",
        "| PER_exponent  | 0/+FLOAT |The expoenet for prioritized experience replay.                           |\n",
        "| episode_for_testing  | +INT |How many epsiodes will your agent run in the testing mode where your agent will not learn offline.                        |\n",
        "| render_for_human  | BOLEAN | Wether you want to render the visual result for each step in the testing mode.              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## blackjack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = 'Blackjack-v1'          #⚠️\n",
        "max_steps_for_each_episode = 10     #⚠️\n",
        "  \n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  201                   #⚠️\n",
        "hidden_size = 250                   #⚠️\n",
        "action_size = 2                     #⚠️\n",
        "time_size = 5                       #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'         \n",
        "bias = False\n",
        "drop_rate = 0.001                   \n",
        "alpha = 0.1                         #⚠️                      \n",
        "iteration_for_learning = 2000       #⚠️                 \n",
        "load_pre_model = False           \n",
        "  \n",
        "  \n",
        "noise_t = 1                        \n",
        "noise_r = 0.1                       #⚠️\n",
        "beta = 0.1                          #⚠️                        \n",
        "iteration_for_deducing = 200        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = 1\n",
        "batch_size_for_offline_learning = 100 \n",
        "PER_epsilon = 0.000001              \n",
        "PER_exponent = 1                                      \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'           #⚠️\n",
        "max_steps_for_each_episode = 2000   #⚠️\n",
        "  \n",
        "  \n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  400                   #⚠️\n",
        "hidden_size = 400                   #⚠️\n",
        "action_size = 2                     #⚠️\n",
        "time_size = 15                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'   \n",
        "bias = False      \n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                         #⚠️                      \n",
        "iteration_for_learning = 1000       #⚠️               \n",
        "load_pre_model = False           \n",
        "  \n",
        "  \n",
        "noise_t = 1                        \n",
        "noise_r = 0.1                 \n",
        "beta = 0.1                          #⚠️                        \n",
        "iteration_for_deducing = 50         #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size                     \n",
        "batch_size_for_offline_learning = 1 \n",
        "PER_epsilon = 0.000001              \n",
        "PER_exponent = 1                                        \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## mountain car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name =  'MountainCar-v0'       #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  200                   #⚠️\n",
        "hidden_size = 200                   #⚠️\n",
        "action_size = 3                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'      \n",
        "bias = False   \n",
        "drop_rate = 0.001                   \n",
        "alpha = 0.1                         #⚠️                         \n",
        "iteration_for_learning = 1000       #⚠️                 \n",
        "load_pre_model = False              \n",
        "  \n",
        "  \n",
        "noise_t = 1                       \n",
        "noise_r = 0.1                       #⚠️\n",
        "beta = 0.1                          #⚠️                     \n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = 2\n",
        "batch_size_for_offline_learning = 10 \n",
        "PER_epsilon = 0.000001              \n",
        "PER_exponent = 1                                        \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## mountain car continuous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = 'MountainCarContinuous-v0'       #⚠️\n",
        "max_steps_for_each_episode = 200             #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  2                     #⚠️\n",
        "hidden_size = 100                   #⚠️\n",
        "action_size = 1                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'      \n",
        "bias = False   \n",
        "drop_rate = 0.001                   \n",
        "alpha = 0.01                        #⚠️                         \n",
        "iteration_for_learning = 1000       #⚠️                 \n",
        "load_pre_model = False              \n",
        "  \n",
        "  \n",
        "noise_t = 1                       \n",
        "noise_r = 0.001                     #⚠️\n",
        "beta = 0.01                         #⚠️                     \n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = 2\n",
        "batch_size_for_offline_learning = 10 \n",
        "PER_epsilon = 0.000001              \n",
        "PER_exponent = 1                               \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = 'Acrobot-v1'            #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "  \n",
        "  \n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  600                   #⚠️\n",
        "hidden_size = 600                   #⚠️\n",
        "action_size = 3                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'   \n",
        "bias = False      \n",
        "drop_rate = 0.001                   \n",
        "alpha = 0.1                         #⚠️                      \n",
        "iteration_for_learning = 1000       #⚠️              \n",
        "load_pre_model = False           \n",
        "  \n",
        "  \n",
        "noise_t = 1                        \n",
        "noise_r = 0.01                      #⚠️\n",
        "beta = 0.1                          #⚠️                       \n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = 2\n",
        "batch_size_for_offline_learning = 10 \n",
        "PER_epsilon = 0.000001              \n",
        "PER_exponent = 1                    \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pendulum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = \"Pendulum-v1\"           #⚠️\n",
        "max_steps_for_each_episode = 100    #⚠️\n",
        "  \n",
        "  \n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  300                   #⚠️\n",
        "hidden_size = 300                   #⚠️\n",
        "action_size = 1                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'   \n",
        "bias = False      \n",
        "drop_rate = 0.001                   \n",
        "alpha = 0.01                        #⚠️                      \n",
        "iteration_for_learning = 1000       #⚠️                 \n",
        "load_pre_model = False           \n",
        "  \n",
        "  \n",
        "noise_t = 1                        \n",
        "noise_r = 0.001                     #⚠️\n",
        "beta = 0.01                         #⚠️                        \n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = 2\n",
        "batch_size_for_offline_learning = 10 \n",
        "PER_epsilon = 0.000001              \n",
        "PER_exponent = 1                    \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## lunar lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v2\"        #⚠️\n",
        "max_steps_for_each_episode = 200    #⚠️\n",
        "\n",
        "\n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  800                   #⚠️\n",
        "hidden_size = 800                   #⚠️\n",
        "action_size = 4                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 250                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'    \n",
        "bias = False     \n",
        "drop_rate = 0.001                   \n",
        "alpha = 0.1                         #⚠️                        \n",
        "iteration_for_learning = 1000       #⚠️    \n",
        "load_pre_model = False        \n",
        "  \n",
        "  \n",
        "noise_t = 1                         \n",
        "noise_r = 0.01                      #⚠️\n",
        "beta = 0.1                          #⚠️      \n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = 2\n",
        "batch_size_for_offline_learning = 10\n",
        "PER_epsilon = 0.000001             \n",
        "PER_exponent = 1                       \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bipedal walker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = 'BipedalWalker-v3'      #⚠️\n",
        "max_steps_for_each_episode = 1000   #⚠️\n",
        "  \n",
        "  \n",
        "ensemble_size = 5                   #◀️\n",
        "state_size =  24                    #⚠️\n",
        "hidden_size = 100                   #⚠️\n",
        "action_size = 4                     #⚠️\n",
        "time_size = 25                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'     \n",
        "bias = False    \n",
        "drop_rate = 0.001                   \n",
        "alpha = 0.01                        #⚠️                      \n",
        "iteration_for_learning = 1000       #⚠️        \n",
        "load_pre_model = False           \n",
        "  \n",
        "  \n",
        "noise_t = 1                        \n",
        "noise_r = 0.001                     #⚠️\n",
        "beta = 0.01                         #⚠️                        \n",
        "iteration_for_deducing = 100        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = 2\n",
        "batch_size_for_offline_learning = 10 \n",
        "PER_epsilon = 0.000001              \n",
        "PER_exponent = 1                                     \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## your present config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'           #⚠️\n",
        "max_steps_for_each_episode = 2000   #⚠️\n",
        "  \n",
        "  \n",
        "ensemble_size = 10                  #◀️\n",
        "state_size =  400                   #⚠️\n",
        "hidden_size = 400                   #⚠️\n",
        "action_size = 2                     #⚠️\n",
        "time_size = 15                      #⚠️\n",
        "reward_size = 100                   #⚠️\n",
        "neural_type = 'gru'                 #⚠️\n",
        "num_layers = 2                      #⚠️\n",
        "num_heads = None                    #⚠️\n",
        "hidden_activation = 'tanh'          \n",
        "output_activation = 'sigmoid'       \n",
        "shift = 0.0                         \n",
        "init = \"random_normal\"              \n",
        "opti = 'sgd'                        \n",
        "loss = 'mean_squared_error'   \n",
        "bias = False      \n",
        "drop_rate = 0.001\n",
        "alpha = 0.1                         #⚠️                      \n",
        "iteration_for_learning = 10000      #⚠️               \n",
        "load_pre_model = False           \n",
        "  \n",
        "  \n",
        "noise_t = 1                        \n",
        "noise_r = 0.1                 \n",
        "beta = 0.1                          #⚠️                        \n",
        "iteration_for_deducing = 500        #⚠️\n",
        "\n",
        "\n",
        "episode_for_training = 100000\n",
        "chunk_size = time_size                     \n",
        "batch_size_for_offline_learning = 10\n",
        "PER_epsilon = 0.000001              \n",
        "PER_exponent = 1                                        \n",
        "\n",
        "\n",
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "suffix                      = f\"game={game_name}_type={neural_type}_ensemble={ensemble_size:05d}_drop={drop_rate:.5f}_learn={iteration_for_learning:05d}_interval={batch_size_for_offline_learning:05d}_deduce={iteration_for_deducing:05d}\"\n",
        "directory                   = f'/content/result/{game_name}/'\n",
        "model_directory             = f'/content/result/{game_name}/model_{suffix}'+'_%s.h5'\n",
        "performance_log_directory   = f'/content/result/{game_name}/performace_log_{suffix}.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing local modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "if   game_name == 'Blackjack-v1':\n",
        "    from envs.env_blackjack   import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif   game_name == 'CartPole-v1':\n",
        "    from envs.env_cartpole    import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'MountainCar-v0':\n",
        "    from envs.env_mountaincar import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'MountainCarContinuous-v0':\n",
        "    from envs.env_mountaincar_continuous import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'Acrobot-v1':\n",
        "    from envs.env_acrobot import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == \"Pendulum-v1\":\n",
        "    from envs.env_pendulum import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == \"LunarLander-v2\":\n",
        "    from envs.env_lunarlander import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "elif game_name == 'BipedalWalker-v3':\n",
        "    from envs.env_bipedalwalker import vectorizing_state, vectorizing_action, vectorizing_reward\n",
        "else:\n",
        "   raise RuntimeError('missing env functions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "if neural_type == 'rnn_att':\n",
        "    from models.model_rnn_att import build_model\n",
        "    from utils.util_rnn_att   import initialize_pre_activated_action, \\\n",
        "                                 update_pre_activated_action, \\\n",
        "                                 sequentialize, \\\n",
        "                                 update_model, \\\n",
        "                                 save_performance_to_csv\n",
        "else:\n",
        "    from models.model_rnn import build_model\n",
        "    from utils.util_rnn   import initialize_pre_activated_action, \\\n",
        "                                 update_pre_activated_action, \\\n",
        "                                 sequentialize, \\\n",
        "                                 update_model, \\\n",
        "                                 save_performance_to_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# Deducing -> Learning\n",
        "Training mode where your agent will learn offline. You can see here how your agent learn overtime and improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "if load_pre_model == False:\n",
        "\n",
        "    model_list = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            shift,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            bias,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_list.append(model)\n",
        "\n",
        "elif load_pre_model == True:\n",
        "\n",
        "    model_list = []\n",
        "    for _ in range(ensemble_size):\n",
        "        model = build_model(state_size,\n",
        "                            hidden_size,\n",
        "                            action_size,\n",
        "                            time_size,\n",
        "                            reward_size,\n",
        "                            neural_type,\n",
        "                            num_layers,\n",
        "                            num_heads,\n",
        "                            hidden_activation,\n",
        "                            output_activation,\n",
        "                            shift,\n",
        "                            init,\n",
        "                            opti,\n",
        "                            loss,\n",
        "                            bias,\n",
        "                            drop_rate,\n",
        "                            alpha)\n",
        "        model.to(device)\n",
        "        model_list.append(model)\n",
        "\n",
        "    for i in range(len(model_list)):\n",
        "        model_list[i].load_state_dict(torch.load( model_directory  % i ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XdQIBpSxeF"
      },
      "source": [
        "Creating Streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Qfp24ueJSxeG"
      },
      "outputs": [],
      "source": [
        "stream_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    stream  = torch.cuda.Stream()\n",
        "    stream_list.append(stream)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6VzvFnSxeH"
      },
      "source": [
        "Creating desired reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "niKHSkhECOE1"
      },
      "outputs": [],
      "source": [
        "desired_reward = torch.ones((1, reward_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "df8f8daf-8e05-41fb-dd81-5dc6983bd6ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "performance_log = []\n",
        "performance_log.append([0, -sys.maxsize])\n",
        "\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    total_steps = 0\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    short_term_state_list  = []\n",
        "    short_term_action_list = []\n",
        "    short_term_reward_list = []\n",
        "\n",
        "    # initializing environment\n",
        "    env                    = gym.make(game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    summed_reward          = 0\n",
        "\n",
        "    # observing state\n",
        "    state    = vectorizing_state(state)\n",
        "    short_term_state_list.append(state)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        # initializing and updating action\n",
        "        state                 = torch.tensor(np.atleast_2d(state), dtype=torch.float)\n",
        "        pre_activated_action  = initialize_pre_activated_action(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_action  = torch.tensor(pre_activated_action[np.newaxis, :, :], dtype=torch.float)\n",
        "        pre_activated_action  = update_pre_activated_action(iteration_for_deducing,\n",
        "                                                            model_list,\n",
        "                                                            state,\n",
        "                                                            pre_activated_action,\n",
        "                                                            desired_reward,\n",
        "                                                            beta,\n",
        "                                                            device)\n",
        "        action, action_       = vectorizing_action(pre_activated_action)\n",
        "        short_term_action_list.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, info = env.step(action_)\n",
        "        total_steps              += 1\n",
        "\n",
        "        # observing actual reward\n",
        "        summed_reward += reward\n",
        "        reward = vectorizing_reward(state, reward, summed_reward, done, reward_size)\n",
        "        short_term_reward_list.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state    = vectorizing_state(state)\n",
        "        short_term_state_list.append(state)\n",
        "\n",
        "        if done: \n",
        "            if  (total_steps >= chunk_size):\n",
        "                print(f'Episode {training_episode+1}: Summed_Reward = {summed_reward}')\n",
        "                performance_log.append([training_episode+1, summed_reward])\n",
        "                save_performance_to_csv(performance_log, performance_log_directory)\n",
        "                break\n",
        "            else:\n",
        "                done = False\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentializing short term experience replay buffer \n",
        "    short_term_sequentialized_state_list   ,\\\n",
        "    short_term_sequentialized_action_list  ,\\\n",
        "    short_term_sequentialized_reward_list  ,\\\n",
        "    short_term_sequentialized_n_state_list = sequentialize(short_term_state_list, short_term_action_list, short_term_reward_list, chunk_size, device)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    if training_episode==0:\n",
        "        long_term_sequentialized_state_list      = copy.deepcopy(short_term_sequentialized_state_list    )\n",
        "        long_term_sequentialized_action_list     = copy.deepcopy(short_term_sequentialized_action_list   )\n",
        "        long_term_sequentialized_reward_list     = copy.deepcopy(short_term_sequentialized_reward_list   )\n",
        "        long_term_sequentialized_n_state_list    = copy.deepcopy(short_term_sequentialized_n_state_list  )\n",
        "    else:\n",
        "        long_term_sequentialized_state_list      = long_term_sequentialized_state_list   + short_term_sequentialized_state_list  \n",
        "        long_term_sequentialized_action_list     = long_term_sequentialized_action_list  + short_term_sequentialized_action_list \n",
        "        long_term_sequentialized_reward_list     = long_term_sequentialized_reward_list  + short_term_sequentialized_reward_list \n",
        "        long_term_sequentialized_n_state_list    = long_term_sequentialized_n_state_list + short_term_sequentialized_n_state_list\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    # batch offline learning\n",
        "    if (training_episode+1) % batch_size_for_offline_learning == 0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # training with Prioritized Experience Replay (PER)\n",
        "        for i, model in enumerate(model_list):\n",
        "            with torch.cuda.stream(stream_list[i]):\n",
        "                model                     = update_model(iteration_for_learning,\n",
        "                                                         long_term_sequentialized_state_list   ,\n",
        "                                                         long_term_sequentialized_action_list  ,\n",
        "                                                         long_term_sequentialized_reward_list  ,\n",
        "                                                         long_term_sequentialized_n_state_list ,\n",
        "                                                         model,\n",
        "                                                         PER_epsilon,\n",
        "                                                         PER_exponent)\n",
        "                model_list[i]             = model\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # saving:\n",
        "        for i in range(len(model_list)):\n",
        "            torch.save(model_list[i].state_dict(), model_directory % i)\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# Deducing only\n",
        "Testing mode where your trained agent in the training mode will not learn offline. It just keeps running each episode without learning new stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        hidden_size,\n",
        "                        action_size,\n",
        "                        time_size,\n",
        "                        reward_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        hidden_activation,\n",
        "                        output_activation,\n",
        "                        shift,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        bias,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "for i in range(len(model_list)):\n",
        "    model_list[i].load_state_dict(torch.load(model_directory % i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRIFvTYSxeJ"
      },
      "source": [
        "Creating desired reward ... again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW24TEH7COE2"
      },
      "outputs": [],
      "source": [
        "desired_reward = torch.ones((1, reward_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "total_summed_reward = 0\n",
        "\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    if render_for_human == True:\n",
        "        env = gym.make( game_name, render_mode=\"human\")\n",
        "    else:\n",
        "        env = gym.make( game_name)\n",
        "    env._max_episode_steps = max_steps_for_each_episode\n",
        "    state                  = env.reset()\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "    summed_reward = 0\n",
        "\n",
        "    state = vectorizing_state(state)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        state                 = torch.tensor(np.atleast_2d(state), dtype=torch.float)\n",
        "        pre_activated_action  = initialize_pre_activated_action(init, noise_t, noise_r, (time_size, action_size))\n",
        "        pre_activated_action  = torch.tensor(pre_activated_action[np.newaxis, :, :], dtype=torch.float)\n",
        "        pre_activated_action  = update_pre_activated_action(iteration_for_deducing,\n",
        "                                                            model_list,\n",
        "                                                            state,\n",
        "                                                            pre_activated_action,\n",
        "                                                            desired_reward,\n",
        "                                                            beta,\n",
        "                                                            device)\n",
        "        action, action_       = vectorizing_action(pre_activated_action)\n",
        "\n",
        "        state, reward, done,  info = env.step(action_)\n",
        "        if render_for_human == True:\n",
        "            env.render()\n",
        "\n",
        "        summed_reward += reward\n",
        "\n",
        "        state = vectorizing_state(state)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfiVv3_J1Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTU0ScHf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHpEEIjf-v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7yADEof-v_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Genrl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
