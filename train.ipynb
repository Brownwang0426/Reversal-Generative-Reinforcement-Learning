{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/Reversal-Generative-Reinforcement-Learning/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Setting up (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4khPQ2_Kf-v1"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10\n",
        "!pip install torch==2.0.1 \n",
        "!pip install numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gymnasium==1.0.0 minigrid==3.0.0 tqdm==4.67.1 dill==0.3.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVZs5loNPbPn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Brownwang0426/Reversal-Generative-Reinforcement-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj-G_J3dPbPo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Reversal-Generative-Reinforcement-Learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KFEnzFfMMF"
      },
      "source": [
        "# Setting up (for local)\n",
        "CUDA Toolkit 11.8 \\\n",
        "cuDNN 8.9.x \\\n",
        "pip install torch==2.0.1 --extra-index-url https://download.pytorch.org/whl/cu118  \\\n",
        "pip install numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gymnasium==1.0.0 minigrid==3.0.0 tqdm==4.67.1 dill==0.3.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import TimeLimit\n",
        "import minigrid\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n",
        "\n",
        "import dill\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import concurrent.futures\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj5V_vlwSxd8"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKNx6iUnXQy"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial configurations regarding how your agent will learn in the environment. The meanings are as follow:\n",
        "(the configs starting with ⚠️ are what we suggest you must tune according to your specific need in your task)\n",
        "(the configs starting with ◀️ are what we suggest you to play with to see the effect)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "\n",
        "## Configs meaning\n",
        "| Configs   | Type   | Description                                                                 |\n",
        "|------------|--------|-----------------------------------------------------------------------------|\n",
        "| ⚠️game_name  | STR| The name of the environment.                                |\n",
        "| ⚠️max_steps_for_each_episode | +INT | The maximun steps that the agent will go through while not done. In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.                    |\n",
        "| ⚠️seed | +INT/None | The seed for environment. None for random environment each episode.                    |\n",
        "| load_pretrained_model  | BOLEAN |Whether you want to load previous trained model.                          |\n",
        "| ◀️ensemble_size  | +INT | The size of the neural ensemble which the agent is comprised of. The bigger, the better, but the longer training time without parallel training. :-D                  |\n",
        "| ⚠️state_size  | +INT | The size of the state as input data.                    |\n",
        "| ⚠️action_size   | +INT | The size of action per step as input data.   |\n",
        "| ⚠️reward_size  | +INT |The size of the reward as output data.                          |\n",
        "| ⚠️feature_size   | +INT |The size of the hidden layers.       |\n",
        "| ⚠️history_size  | 0/+INT |How many steps in the history for state and action will the agent take into consideration.                           |\n",
        "| ⚠️future_size  | +INT |The length of the sequence of actions. Namely, how many steps in the future the agent will predict or use to discern the present best action.                |\n",
        "| ⚠️window_size  | +INT | window size for sequentializing short term experience replay buffer into long term. **`Shall be less than the possible minimum steps in a episode and shall be less or equal to future_size.`**             |\n",
        "| ⚠️neural_type  | STR |  [**`rnn`**, **`gru`**, **`lstm`**, **`td`**, **`rnn_td`**, **`gru_td`**, **`lstm_td`**] The type of neural network you prefer. For now, we support rnn, gru, lstm, and td (Transformer decoder only). More to come in the future (or you can build one yourself :-D in the models repository).           |\n",
        "| ⚠️num_layers  | +INT |The number of layers in rnn, gru, lstm, and td (Transformer decoder only).     |\n",
        "| ⚠️num_heads  | +INT/None |The number of heads in multi-head attention. **`Shall be able to devide feature_size`**. **`Shall be None for non-attention neural_type`**.                         |\n",
        "| init   | STR | [**`random_normal`**, **`random_uniform`**, **`xavier_normal`**, **`xavier_uniform`**, **`glorot_normal`**, **`glorot_uniform`**] The initialization method you prefer for initiating neural net ensemble of your agent.                          |\n",
        "| opti   | STR | [**`adam`**, **`sgd`**, **`rmsprop`**]  The optimization method you prefer.             |\n",
        "| loss  | STR | [**`mean_squared_error`**, **`binary_crossentropy`**] The loss or error function you prefer.                           |\n",
        "| bias  | BOLEAN |Whether you want add bias.                          |\n",
        "| drop_rate   | 0/+FLOAT |The drop-rate for drop-out.              |\n",
        "| alpha   | 0/+FLOAT |The learning rate for neural networks weight matrices.                           |\n",
        "| epoch_for_learning   | +INT |The iteration for learning per experience.              |\n",
        "| init_   | STR | [**`random_normal`**, **`random_uniform`**, **`xavier_normal`**, **`xavier_uniform`**, **`glorot_normal`**, **`glorot_uniform`**] The initialization method you prefer for initiating actions of your agent.                         |\n",
        "| greed_epsilon_t  |  +INT |The times applying gaussian noise to the initializated actions of the agent, similar to diffusion model's adding gaussian noise.          |\n",
        "| greed_epsilon_init  |  +FLOAT |The initial greed_epsilon or noise range to initializate the actions of the agent. The higher the value is, the more exploration-oriented the agent will be in the begining.                    |\n",
        "| greed_epsilon_decay  |  +FLOAT | The rate of decaying for greed_epsilon for each step and eposide.  |\n",
        "| greed_epsilon_min  |  +FLOAT |A very small number representing the lower bound of the greed_epsilon.        |\n",
        "| beta  |  0/+FLOAT |The updating rate for updating actions of the agent.              |\n",
        "| ◀️epoch_for_planning  |  +INT |The iteration for updating actions of the agent.                           |\n",
        "| episode_for_training  | +INT |How many epsiodes will your agent run in the training mode where your agent will learn offline.              |\n",
        "| batch_size_for_planing  | +INT | How many neural ents will be merged into a batch for each iteration in planning. **`Not usaed for the present time`**.              |\n",
        "| ⚠️batch_size_for_executing| +INT | How many steps will the agent skip planning and simply take actions planned before. **`Shall be less or equal to future_size`**. |\n",
        "| ⚠️batch_size_for_learning  | +INT | Batch size for learning or training neural nets.              |\n",
        "| buffer_limit  | +INT |The maximum size for your buffer.              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTeSFTdvna7n"
      },
      "source": [
        "\n",
        "## dualism principles\n",
        "\n",
        "| neural weights | neural actions (and experiences) |\n",
        "|----------|----------|\n",
        "| Re-updated   | Not re-updated. New ones are created and stored each time   |\n",
        "| Stable initialization   | Unstable initialization at first and then stable initialization gradually   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ZJ8yAtPbPw"
      },
      "source": [
        "## frozen lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAvATrJke-sI"
      },
      "outputs": [],
      "source": [
        "game_name =  'FrozenLake-v1'         #⚠️   gym.make(game_name, max_episode_steps=max_steps_for_each_episode, is_slippery=False, map_name=\"4x4\")\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  16                     #⚠️\n",
        "action_size = 4                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 200                   #⚠️\n",
        "history_size  = 10                   #⚠️\n",
        "future_size = 10                     #⚠️\n",
        "window_size = 2                      #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                   \n",
        "epoch_for_learning  = 5              \n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 0.1       \n",
        "greed_epsilon_decay = 0.95                 \n",
        "greed_epsilon_min   = 1e-20        \n",
        "beta = 1                         \n",
        "epoch_for_planning = 5               #◀️          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "batch_size_for_executing = 10        #⚠️\n",
        "\n",
        "batch_size_for_learning = 10         #⚠️  \n",
        "\n",
        "buffer_limit = 10000                       \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASBb8wJPbPw"
      },
      "source": [
        "## blackjack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WpWlvc8e-sI"
      },
      "outputs": [],
      "source": [
        "game_name = 'Blackjack-v1'           #⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  201                    #⚠️\n",
        "action_size = 2                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 250                   #⚠️\n",
        "history_size  = 5                    #⚠️\n",
        "future_size = 5                      #⚠️\n",
        "window_size = 2                      #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                   \n",
        "epoch_for_learning  = 5              \n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 0.1       \n",
        "greed_epsilon_decay = 0.95                 \n",
        "greed_epsilon_min   = 1e-20        \n",
        "beta = 1                         \n",
        "epoch_for_planning = 5               #◀️          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "batch_size_for_executing = 10        #⚠️\n",
        "\n",
        "batch_size_for_learning = 10         #⚠️     \n",
        "\n",
        "buffer_limit = 10000                       \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgCIHGHWPbPx"
      },
      "source": [
        "## cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_JgJ4E3e-sJ"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'            #⚠️\n",
        "max_steps_for_each_episode = 1000    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  400                    #⚠️\n",
        "action_size = 2                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 500                   #⚠️\n",
        "history_size  = 10                   #⚠️\n",
        "future_size = 50                     #⚠️\n",
        "window_size = 8                      #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                   \n",
        "epoch_for_learning  = 5              \n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 0.1       \n",
        "greed_epsilon_decay = 0.95                 \n",
        "greed_epsilon_min   = 1e-20        \n",
        "beta = 1                         \n",
        "epoch_for_planning = 5               #◀️          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "batch_size_for_executing = 10        #⚠️\n",
        "\n",
        "batch_size_for_learning = 10         #⚠️       \n",
        "\n",
        "buffer_limit = 10000                       \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MiLL2IAPbPx"
      },
      "source": [
        "## mountain car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcI2SNuje-sJ"
      },
      "outputs": [],
      "source": [
        "game_name =  'MountainCar-v0'        #⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  200                    #⚠️\n",
        "action_size = 3                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 300                   #⚠️\n",
        "history_size  = 25                   #⚠️\n",
        "future_size = 75                     #⚠️\n",
        "window_size = 50                     #⚠️   \n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                   \n",
        "epoch_for_learning  = 5              \n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 0.1       \n",
        "greed_epsilon_decay = 0.95                 \n",
        "greed_epsilon_min   = 1e-20        \n",
        "beta = 1                         \n",
        "epoch_for_planning = 5               #◀️          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "batch_size_for_executing = 10        #⚠️\n",
        "\n",
        "batch_size_for_learning = 10         #⚠️        \n",
        "\n",
        "buffer_limit = 10000                       \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75C-eQ0QPbPy"
      },
      "source": [
        "## acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stJnWGvoe-sK"
      },
      "outputs": [],
      "source": [
        "game_name = 'Acrobot-v1'             #⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  600                    #⚠️\n",
        "action_size = 3                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 700                   #⚠️\n",
        "history_size  = 25                   #⚠️\n",
        "future_size = 75                     #⚠️\n",
        "window_size = 50                     #⚠️   \n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                   \n",
        "epoch_for_learning  = 5              \n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 0.1       \n",
        "greed_epsilon_decay = 0.95                 \n",
        "greed_epsilon_min   = 1e-20        \n",
        "beta = 1                         \n",
        "epoch_for_planning = 5               #◀️          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "batch_size_for_executing = 10        #⚠️        \n",
        "\n",
        "batch_size_for_learning = 10         #⚠️  \n",
        "\n",
        "buffer_limit = 10000                       \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yi3S50oPbPy"
      },
      "source": [
        "## lunar lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTcH-Fk7e-sK"
      },
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v3\"         #⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  800                    #⚠️\n",
        "action_size = 4                      #⚠️\n",
        "reward_size = 250                    #⚠️\n",
        "feature_size = 950                   #⚠️\n",
        "history_size  = 25                   #⚠️\n",
        "future_size = 75                     #⚠️\n",
        "window_size = 50                     #⚠️   \n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                   \n",
        "epoch_for_learning  = 5              \n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 0.1       \n",
        "greed_epsilon_decay = 0.95                 \n",
        "greed_epsilon_min   = 1e-20        \n",
        "beta = 1                         \n",
        "epoch_for_planning = 5               #◀️          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "batch_size_for_executing = 10        #⚠️          \n",
        "\n",
        "batch_size_for_learning = 10         #⚠️ \n",
        "\n",
        "buffer_limit = 10000                       \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNIMnHzKk8eV"
      },
      "source": [
        "## door key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYQjUKJue-sK"
      },
      "outputs": [],
      "source": [
        "game_name = \"MiniGrid-DoorKey-5x5-v0\"#⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = 1                             #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  157                    #⚠️\n",
        "action_size = 7                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 250                   #⚠️\n",
        "history_size  = 15                   #⚠️\n",
        "future_size = 10                     #⚠️\n",
        "window_size = 10                     #⚠️   \n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                   \n",
        "epoch_for_learning  = 5              \n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 0.1       \n",
        "greed_epsilon_decay = 0.95                 \n",
        "greed_epsilon_min   = 1e-20        \n",
        "beta = 1                         \n",
        "epoch_for_planning = 5               #◀️          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "batch_size_for_executing = 1         #⚠️         \n",
        "\n",
        "batch_size_for_learning = 10         #⚠️  \n",
        "\n",
        "buffer_limit = 10000                       \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wyup25fPbPz"
      },
      "source": [
        "## your present config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ounGFZUe-sL"
      },
      "outputs": [],
      "source": [
        "game_name =  'MountainCar-v0'        #⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  200                    #⚠️\n",
        "action_size = 3                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 300                   #⚠️\n",
        "history_size  = 25                   #⚠️\n",
        "future_size = 75                     #⚠️\n",
        "window_size = 50                     #⚠️   \n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                   \n",
        "epoch_for_learning  = 5              \n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_init  = 0.1       \n",
        "greed_epsilon_decay = 0.95                 \n",
        "greed_epsilon_min   = 1e-20        \n",
        "beta = 1                         \n",
        "epoch_for_planning = 5               #◀️          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "batch_size_for_executing = 10        #⚠️\n",
        "\n",
        "batch_size_for_learning = 10         #⚠️        \n",
        "\n",
        "buffer_limit = 10000                       \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0p92vcre-sL"
      },
      "outputs": [],
      "source": [
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "suffix                 = f\"game_{game_name}-type_{neural_type}-ensemble_{ensemble_size:05d}-learn_{epoch_for_learning:05d}-plan_{epoch_for_planning:05d}\"\n",
        "directory              = f'./result/{game_name}/'\n",
        "performance_directory  = f'./result/{game_name}/performace-{suffix}.csv'\n",
        "model_directory        = f'./result/{game_name}/model-{suffix}.pth'\n",
        "buffer_directory       = f'./result/{game_name}/buffer-{suffix}.dill'\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TcwUHixPbPz"
      },
      "source": [
        "# Importing local modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUk1dHbfPbPz"
      },
      "outputs": [],
      "source": [
        "game_modules = {\n",
        "    'FrozenLake-v1': 'envs.env_frozenlake',\n",
        "    'Blackjack-v1': 'envs.env_blackjack',\n",
        "    'CartPole-v1': 'envs.env_cartpole',\n",
        "    'MountainCar-v0': 'envs.env_mountaincar',\n",
        "    'Acrobot-v1': 'envs.env_acrobot',\n",
        "    'LunarLander-v3': 'envs.env_lunarlander',\n",
        "    'MiniGrid-DoorKey-5x5-v0': 'envs.env_doorkey'\n",
        "}\n",
        "if game_name in game_modules:\n",
        "    game_module = __import__(game_modules[game_name], fromlist=['vectorizing_state', 'vectorizing_action', 'vectorizing_reward'])\n",
        "    vectorizing_state  = game_module.vectorizing_state\n",
        "    vectorizing_action = game_module.vectorizing_action\n",
        "    vectorizing_reward = game_module.vectorizing_reward\n",
        "else:\n",
        "    raise RuntimeError('Missing env functions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErBxWJc2PbP0"
      },
      "outputs": [],
      "source": [
        "model_modules = {\n",
        "    'td': 'models.model_td',\n",
        "    'rnn_td': 'models.model_rnn_td',\n",
        "    'gru_td': 'models.model_gru_td',\n",
        "    'rnn_td': 'models.model_rnn_td',\n",
        "    'rnn': 'models.model_rnn',\n",
        "    'gru': 'models.model_rnn',\n",
        "    'lstm': 'models.model_rnn'\n",
        "}\n",
        "if neural_type in model_modules:\n",
        "    model_module = __import__(model_modules[neural_type], fromlist=['build_model'])\n",
        "    build_model  = model_module.build_model\n",
        "else:\n",
        "    raise RuntimeError('Missing model functions')\n",
        "\n",
        "from utils.util_func  import load_performance_from_csv,\\\n",
        "                             load_buffer_from_pickle,\\\n",
        "                             retrieve_history,\\\n",
        "                             retrieve_present,\\\n",
        "                             initialize_future_action, \\\n",
        "                             initialize_desired_reward,\\\n",
        "                             update_future_action, \\\n",
        "                             sequentialize, \\\n",
        "                             update_long_term_experience_replay_buffer,\\\n",
        "                             update_model_list,\\\n",
        "                             limit_buffer,\\\n",
        "                             save_performance_to_csv,\\\n",
        "                             save_buffer_to_pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# planning -> Learning\n",
        "Training mode where your agent will learn offline. You can see here how your agent learn overtime and improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "## Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# creating empty log for recording performance\n",
        "performance_log  = []\n",
        "\n",
        "# setting the last episode number for performance log\n",
        "last_episode = 0\n",
        "\n",
        "# creating model list\n",
        "sequence_size = history_size + future_size \n",
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        action_size,\n",
        "                        reward_size,\n",
        "                        feature_size,\n",
        "                        sequence_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        bias,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "# creating space for storing tensors as experience replay buffer\n",
        "history_state_stack        = torch.empty(0).to(device)\n",
        "history_action_stack       = torch.empty(0).to(device)\n",
        "present_state_stack        = torch.empty(0).to(device)\n",
        "future_action_stack        = torch.empty(0).to(device)\n",
        "future_reward_stack        = torch.empty(0).to(device)\n",
        "future_state_stack         = torch.empty(0).to(device)\n",
        "history_state_hash_list    = list()\n",
        "history_action_hash_list   = list()\n",
        "present_state_hash_list    = list()\n",
        "future_action_hash_list    = list()\n",
        "future_reward_hash_list    = list()\n",
        "future_state_hash_list     = list()\n",
        "\n",
        "# load from pre-trained models if needed\n",
        "if load_pretrained_model == False:\n",
        "    pass\n",
        "elif load_pretrained_model == True:\n",
        "    try:\n",
        "        model_dict = torch.load(model_directory)\n",
        "        for i, model in enumerate(model_list):\n",
        "            model.load_state_dict(model_dict[f'model_{i}'])\n",
        "        history_state_stack, \\\n",
        "        history_action_stack, \\\n",
        "        present_state_stack, \\\n",
        "        future_action_stack, \\\n",
        "        future_reward_stack, \\\n",
        "        future_state_stack , \\\n",
        "        history_state_hash_list, \\\n",
        "        history_action_hash_list, \\\n",
        "        present_state_hash_list, \\\n",
        "        future_action_hash_list, \\\n",
        "        future_reward_hash_list, \\\n",
        "        future_state_hash_list = load_buffer_from_pickle(buffer_directory)\n",
        "        history_state_stack    = history_state_stack.to (device) \n",
        "        history_action_stack   = history_action_stack.to(device) \n",
        "        present_state_stack    = present_state_stack.to (device) \n",
        "        future_action_stack    = future_action_stack.to (device) \n",
        "        future_reward_stack    = future_reward_stack.to (device) \n",
        "        future_state_stack     = future_state_stack .to (device) \n",
        "        performance_log        = load_performance_from_csv(performance_directory)\n",
        "        last_episode           = performance_log[-1][0] + 1 if len(performance_log) > 0 else 0\n",
        "        greed_epsilon_init     = max(greed_epsilon_init * (greed_epsilon_decay ** last_episode), greed_epsilon_min)\n",
        "        print('Loaded pre-trained models.')\n",
        "    except:\n",
        "        print('Failed loading pre-trained models. Now using new models.')\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "## Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "41cc7bf4-4028-4a3e-b795-2c52d25bcced"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We don't randomize desired reward anymore because:\n",
        "1 - It is not typical in RL.\n",
        "2 - There are many more effective methods like epsilon-greedy, intrinsic motivation, and reward shaping that can drive an agent to explore effectively.\n",
        "3 - Those methods are designed to balance exploration and exploitation in a way that promotes learning while keeping the agent on a meaningful path toward mastering the environment.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "We no longer use Prioritized Experience Replay (PER) but rather use Random Experience Replay because:\n",
        "1 - Although the agent might not learn as quickly as with Prioritized Experience Replay (PER), random experience replay allows the agent to gradually improve and become more stable. Over time, this leads to better long-term performance.\n",
        "2 - Since random experience replay avoids focusing on just the most \"critical\" experiences, it reduces the risk of instability or overfitting. The agent ends up with a more robust and reliable policy that performs well in a variety of situations.\n",
        "3 - PER is GPU VRAM killer\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "planning phase:\n",
        "    (history_state) (history_action) present_state         future_action                 desired_rewar\n",
        "                                     -observed by agent    -planned by agent             -planned by agent\n",
        "learning phase:\n",
        "                                     present_state         future_action                 future_reward             future_state\n",
        "                                     -observed by agent    -observed/executed by agent   -observed by agent        -observed by agent\n",
        "                                                                                         -criterion set by human\n",
        "\"\"\"\n",
        "\n",
        "# starting each episode\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "\n",
        "    # initializing summed reward\n",
        "    summed_reward  = 0\n",
        "\n",
        "    \"\"\"\n",
        "    We filled short term experience replay buffer with some dummy data to insure that history exists.\n",
        "    \"\"\"\n",
        "    # initializing short term experience replay buffer\n",
        "    state_list  = []\n",
        "    action_list = []\n",
        "    reward_list = []\n",
        "    for _ in range(history_size):\n",
        "        state_list .append(torch.zeros(state_size  ).to(device) - 1)\n",
        "        action_list.append(torch.zeros(action_size ).to(device)    )\n",
        "        reward_list.append(torch.zeros(reward_size ).to(device)    ) \n",
        "\n",
        "    # initializing environment\n",
        "    env            = gym.make(game_name, max_episode_steps=max_steps_for_each_episode)\n",
        "    state, info    = env.reset(seed = seed)\n",
        "    \n",
        "    # observing state\n",
        "    state          = vectorizing_state(state, device)\n",
        "    state_list.append(state)\n",
        "\n",
        "    # starting each step\n",
        "    done = False\n",
        "    truncated = False \n",
        "    while not done and not truncated:\n",
        "\n",
        "        \"\"\"\n",
        "        We let agent took some history states and actions into consideration.\n",
        "        \"\"\"\n",
        "        # initializing and updating action by desired reward                                  \n",
        "        history_state, \\\n",
        "        history_action  = retrieve_history(state_list, action_list, history_size, device)\n",
        "        present_state   = retrieve_present(state_list, device)\n",
        "        future_action   = initialize_future_action(init_, greed_epsilon_t, greed_epsilon_init, (1, future_size, action_size), device)\n",
        "        desired_reward  = initialize_desired_reward((1, future_size, reward_size), device)\n",
        "        future_action   = update_future_action(epoch_for_planning,\n",
        "                                               model_list,\n",
        "                                               history_state ,\n",
        "                                               history_action,\n",
        "                                               present_state,\n",
        "                                               future_action,\n",
        "                                               desired_reward,\n",
        "                                               beta)\n",
        "\n",
        "        \"\"\"\n",
        "        We let agent execute several planned actions rather than one at a time to make data gathering more efficient. \n",
        "        batch_size_for_executing shall be less or equal to future_size.\n",
        "        \"\"\"\n",
        "        # taking actions and skip planning \n",
        "        for i in range(batch_size_for_executing):\n",
        "\n",
        "            # printing steps\n",
        "            print(f'\\rStep: {len(action_list)+1-history_size}\\r', end='', flush=True)\n",
        "\n",
        "            # observing action\n",
        "            action, action_  = vectorizing_action(future_action[:, i:, :], device)\n",
        "            action_list.append(action)\n",
        "            \n",
        "            # executing action\n",
        "            state, reward, done, truncated, info = env.step(action_)\n",
        "\n",
        "            # summing reward\n",
        "            summed_reward += reward\n",
        "\n",
        "            # observing actual reward\n",
        "            reward = vectorizing_reward(state, reward, summed_reward, done, reward_size, device)\n",
        "            reward_list.append(reward)\n",
        "\n",
        "            # observing state\n",
        "            state  = vectorizing_state(state, device)\n",
        "            state_list.append(state)\n",
        "\n",
        "            # if done then continue for a short period\n",
        "            if done or truncated:\n",
        "                break\n",
        "            else:\n",
        "                pass\n",
        "            \n",
        "    # closing env\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # recording performance\n",
        "    print(f'Episode {training_episode + last_episode}: Summed_Reward = {summed_reward}')\n",
        "    performance_log.append([training_episode + last_episode, summed_reward])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    window_size shall be less or equal to future_size and shall be smaller or equal to the possible minimum steps in an env.\n",
        "    \"\"\"\n",
        "    # sequentializing short term experience replay buffer\n",
        "    history_state_list   ,\\\n",
        "    history_action_list   ,\\\n",
        "    present_state_list   ,\\\n",
        "    future_action_list   ,\\\n",
        "    future_reward_list   ,\\\n",
        "    future_state_list    = sequentialize(state_list  ,\n",
        "                                         action_list ,\n",
        "                                         reward_list ,\n",
        "                                         history_size,\n",
        "                                         window_size)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    # storing sequentialized short term experience to long term experience replay buffer when it is a new experience\n",
        "    history_state_stack, \\\n",
        "    history_action_stack, \\\n",
        "    present_state_stack, \\\n",
        "    future_action_stack, \\\n",
        "    future_reward_stack, \\\n",
        "    future_state_stack , \\\n",
        "    history_state_hash_list  , \\\n",
        "    history_action_hash_list  , \\\n",
        "    present_state_hash_list  , \\\n",
        "    future_action_hash_list  , \\\n",
        "    future_reward_hash_list  , \\\n",
        "    future_state_hash_list   = update_long_term_experience_replay_buffer(history_state_stack,\n",
        "                                                                         history_action_stack,\n",
        "                                                                         present_state_stack,\n",
        "                                                                         future_action_stack,\n",
        "                                                                         future_reward_stack,\n",
        "                                                                         future_state_stack ,\n",
        "                                                                         history_state_hash_list  ,\n",
        "                                                                         history_action_hash_list  ,\n",
        "                                                                         present_state_hash_list  ,\n",
        "                                                                         future_action_hash_list  ,\n",
        "                                                                         future_reward_hash_list  ,\n",
        "                                                                         future_state_hash_list   ,\n",
        "                                                                         history_state_list   ,\n",
        "                                                                         history_action_list   ,\n",
        "                                                                         present_state_list,\n",
        "                                                                         future_action_list,\n",
        "                                                                         future_reward_list,\n",
        "                                                                         future_state_list )\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \"\"\"\n",
        "    We use batch_size to make training more efficient.\n",
        "    \"\"\"\n",
        "    # training\n",
        "    model_list = update_model_list(epoch_for_learning ,\n",
        "                                   history_state_stack,\n",
        "                                   history_action_stack,\n",
        "                                   present_state_stack,\n",
        "                                   future_action_stack,\n",
        "                                   future_reward_stack,\n",
        "                                   future_state_stack ,\n",
        "                                   model_list,\n",
        "                                   batch_size_for_learning\n",
        "                                   )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # limit_buffer\n",
        "    history_state_stack, \\\n",
        "    history_action_stack, \\\n",
        "    present_state_stack, \\\n",
        "    future_action_stack, \\\n",
        "    future_reward_stack, \\\n",
        "    future_state_stack , \\\n",
        "    history_state_hash_list  , \\\n",
        "    history_action_hash_list  , \\\n",
        "    present_state_hash_list  , \\\n",
        "    future_action_hash_list  , \\\n",
        "    future_reward_hash_list  , \\\n",
        "    future_state_hash_list   = limit_buffer(history_state_stack,\n",
        "                                            history_action_stack,\n",
        "                                            present_state_stack,\n",
        "                                            future_action_stack,\n",
        "                                            future_reward_stack,\n",
        "                                            future_state_stack ,\n",
        "                                            history_state_hash_list  ,\n",
        "                                            history_action_hash_list  ,\n",
        "                                            present_state_hash_list  ,\n",
        "                                            future_action_hash_list  ,\n",
        "                                            future_reward_hash_list  ,\n",
        "                                            future_state_hash_list ,\n",
        "                                            buffer_limit  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    We set a lower bound for greed_epsilon_init to prevent it from becoming too small which is similar to initialzing the weights in neural networks to nearly zero.\n",
        "    \"\"\"\n",
        "    # decreasing decay rate\n",
        "    greed_epsilon_init = greed_epsilon_init * greed_epsilon_decay\n",
        "    greed_epsilon_init = max(greed_epsilon_init , greed_epsilon_min)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # saving final reward to log\n",
        "    save_performance_to_csv(performance_log, performance_directory)\n",
        "\n",
        "    # saving nn models\n",
        "    model_dict = {}\n",
        "    for i, model in enumerate(model_list):\n",
        "        model_dict[f'model_{i}'] = model.state_dict()\n",
        "    torch.save(model_dict, model_directory)\n",
        "\n",
        "    # saving long term experience replay buffer\n",
        "    save_buffer_to_pickle(buffer_directory,\n",
        "                          history_state_stack,\n",
        "                          history_action_stack,\n",
        "                          present_state_stack,\n",
        "                          future_action_stack,\n",
        "                          future_reward_stack,\n",
        "                          future_state_stack,\n",
        "                          history_state_hash_list,\n",
        "                          history_action_hash_list,\n",
        "                          present_state_hash_list,\n",
        "                          future_action_hash_list,\n",
        "                          future_reward_hash_list,\n",
        "                          future_state_hash_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # clear up\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# planning only\n",
        "Testing mode where your trained agent in the training mode will not learn offline. It just keeps running each episode without learning new stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "## Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "sequence_size = history_size + future_size \n",
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        action_size,\n",
        "                        reward_size,\n",
        "                        feature_size,\n",
        "                        sequence_size ,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        bias,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "model_dict = torch.load(model_directory)\n",
        "for i, model in enumerate(model_list):\n",
        "    model.load_state_dict(model_dict[f'model_{i}'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "## Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "# score recorder\n",
        "total_summed_reward = 0\n",
        "\n",
        "# starting each episode\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    # initializing summed reward\n",
        "    summed_reward  = 0\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    state_list  = []\n",
        "    action_list = []\n",
        "    for _ in range(history_size):\n",
        "        state_list .append(torch.zeros(state_size  ).to(device) - 1)\n",
        "        action_list.append(torch.zeros(action_size ).to(device)    )\n",
        "\n",
        "    # initializing environment\n",
        "    env = gym.make(game_name, max_episode_steps = max_steps_for_each_episode,\n",
        "                   render_mode = \"human\" if render_for_human else None)\n",
        "    state, info = env.reset(seed = seed)\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "\n",
        "    # observing state\n",
        "    state = vectorizing_state(state, device)\n",
        "    state_list.append(state)\n",
        "\n",
        "    # starting each step\n",
        "    done = False\n",
        "    truncated = False\n",
        "    while not done and not truncated:\n",
        "        \n",
        "        # initializing and updating action   \n",
        "        history_state, \\\n",
        "        history_action = retrieve_history(state_list, action_list, history_size, device)\n",
        "        present_state  = retrieve_present(state_list, device)\n",
        "        future_action  = initialize_future_action(init_, greed_epsilon_t, greed_epsilon_init, (1, future_size, action_size), device)\n",
        "        desired_reward = initialize_desired_reward((1, future_size, reward_size), device)\n",
        "        future_action  = update_future_action(epoch_for_planning,\n",
        "                                              model_list,\n",
        "                                              history_state ,\n",
        "                                              history_action,\n",
        "                                              present_state,\n",
        "                                              future_action,\n",
        "                                              desired_reward,\n",
        "                                              beta)\n",
        "    \n",
        "         # taking actions and skip planning \n",
        "        for i in range(batch_size_for_executing):\n",
        "\n",
        "            print(f'\\rStep: {len(action_list)+1}\\r', end='', flush=True)\n",
        "\n",
        "            # observing action\n",
        "            action, action_  = vectorizing_action(future_action[:, i:, :], device)\n",
        "            action_list.append(action)\n",
        "\n",
        "            # executing action\n",
        "            state, reward, done, truncated, info = env.step(action_)\n",
        "            if render_for_human == True:\n",
        "                env.render()\n",
        "                \n",
        "            # summing reward\n",
        "            summed_reward += reward\n",
        "            \n",
        "            # observing state\n",
        "            state = vectorizing_state(state, device)\n",
        "            state_list.append(state)\n",
        "            \n",
        "            # terminating episode if done or truncated\n",
        "            if done or truncated:\n",
        "                break\n",
        "            else:\n",
        "                pass\n",
        "        \n",
        "    # closing env\n",
        "    env.close()\n",
        "\n",
        "    # recording\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
