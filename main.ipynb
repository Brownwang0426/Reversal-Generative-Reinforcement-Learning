{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/Reversal-Generative-Reinforcement-Learning/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Setting up (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4khPQ2_Kf-v1"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install python3.10\n",
        "!pip install torch==2.0.1 \n",
        "!pip install numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gymnasium==1.0.0 minigrid==3.0.0 tqdm==4.67.1 dill==0.3.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVZs5loNPbPn"
      },
      "outputs": [],
      "source": [
        "!git clone --branch main https://github.com/Brownwang0426/Reversal-Generative-Reinforcement-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj-G_J3dPbPo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Reversal-Generative-Reinforcement-Learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KFEnzFfMMF"
      },
      "source": [
        "# Setting up (for local)\n",
        "CUDA Toolkit 11.8 \\\n",
        "cuDNN 8.9.x \\\n",
        "pip install torch==2.0.1 --extra-index-url https://download.pytorch.org/whl/cu118  \\\n",
        "pip install numpy==1.25.2 scipy==1.11.4 swig==4.2.1 ufal.pybox2d==2.3.10.3 gymnasium==1.0.0 minigrid==3.0.0 tqdm==4.67.1 dill==0.3.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import TimeLimit\n",
        "import minigrid\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n",
        "\n",
        "import dill\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import concurrent.futures\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj5V_vlwSxd8"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKNx6iUnXQy"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial configurations regarding how your agent will learn in the environment. The meanings are as follow:\n",
        "(the configs starting with ⚠️ are what we suggest you must tune according to your specific need in your task)\n",
        "(the configs starting with ◀️ are what we suggest you to play with to see the effect)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "\n",
        "## Configs meaning\n",
        "| Configs   | Type   | Description                                                                 |\n",
        "|------------|--------|-----------------------------------------------------------------------------|\n",
        "| ⚠️game_name  | STR| The name of the environment.                                |\n",
        "| ⚠️max_steps_for_each_episode | +INT | The maximun steps that the agent will go through while not done. In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.                    |\n",
        "| ⚠️seed | +INT/None | The seed for environment. None for random environment each episode.                    |\n",
        "| load_pretrained_model  | BOLEAN |Whether you want to load previous trained model.                          |\n",
        "| ◀️ensemble_size  | +INT | The size of the neural ensemble which the agent is comprised of. The bigger, the better, but the longer training time without parallel training. :-D                  |\n",
        "| ⚠️state_size  | +INT | The size of the state as input data.                    |\n",
        "| ⚠️action_size   | +INT | The size of action per step as input data.   |\n",
        "| ⚠️reward_size  | +INT |The size of the reward as output data.                          |\n",
        "| ⚠️feature_size   | +INT |The size of the hidden layers. **`Shall be bigger than the sum of state_size, action_size and reward_size`**.      |\n",
        "| ⚠️history_size  | 0/+INT |How many steps in the history for state and action will the agent take into consideration.                           |\n",
        "| ⚠️future_size  | +INT |The length of the sequence of actions. Namely, how many steps in the future the agent will predict or use to discern the present best action.                |\n",
        "| ⚠️neural_type  | STR |  [**`rnn`**, **`gru`**, **`lstm`**, **`td`**, **`rnn_td`**, **`gru_td`**, **`lstm_td`**] The type of neural network you prefer. For now, we support rnn, gru, lstm, and td (Transformer decoder only). More to come in the future (or you can build one yourself :-D in the models repository).           |\n",
        "| ⚠️num_layers  | +INT |The number of layers in rnn, gru, lstm, and td (Transformer decoder only).     |\n",
        "| ⚠️num_heads  | +INT/None |The number of heads in multi-head attention. **`Shall be able to devide feature_size`**. **`Shall be None for non-attention neural_type`**.                         |\n",
        "| init   | STR | [**`random_normal`**, **`random_uniform`**, **`xavier_normal`**, **`xavier_uniform`**, **`glorot_normal`**, **`glorot_uniform`**] The initialization method you prefer for initiating neural net ensemble of your agent.                          |\n",
        "| opti   | STR | [**`adam`**, **`sgd`**, **`rmsprop`**]  The optimization method you prefer.             |\n",
        "| loss  | STR | [**`mean_squared_error`**, **`binary_crossentropy`**] The loss or error function you prefer.                           |\n",
        "| bias  | BOLEAN |Whether you want add bias.                          |\n",
        "| drop_rate   | 0/+FLOAT |The drop-rate for drop-out.              |\n",
        "| alpha   | 0/+FLOAT |The learning rate for neural networks weight matrices.                           |\n",
        "| itrtn_for_learning   | +INT |The iteration for learning per experience.              |\n",
        "| init_   | STR | [**`random_normal`**, **`random_uniform`**] The initialization method you prefer for initiating actions of your agent.                         |\n",
        "| greed_epsilon_t  |  +INT |The times applying gaussian noise to the initializated actions of the agent, similar to diffusion model's adding gaussian noise.          |\n",
        "| greed_epsilon_r  |  +FLOAT |The initial greed_epsilon or noise range to initializate the actions of the agent. The higher the value is, the more exploration-oriented the agent will be in the begining.                    |\n",
        "| ⚠️greed_epsilon_decay| \t+FLOAT\t|The rate of decaying for greed_epsilon for each step and eposide.|\n",
        "| greed_epsilon_min  | \t+FLOAT\t|A very small number representing the lower bound of the greed_epsilon.|\n",
        "| beta  |  0/+FLOAT |The updating rate for updating actions of the agent.              |\n",
        "| itrtn_for_planning  |  +INT |The iteration for updating actions of the agent.                           |\n",
        "| episode_for_training  | +INT |How many epsiodes will your agent run in the training mode where your agent will learn offline.              |\n",
        "| episode_for_validation  | +INT |How many epsiodes will your agent start from regular starting point for validating the actual perfromance of the agent.              |\n",
        "| ⚠️batch_size_for_executing| +INT | How many steps will the agent skip planning and simply take actions planned before. **`Shall be less or equal to future_size`**. |\n",
        "| ⚠️batch_size_for_learning  | +INT | Batch size for learning or training neural nets.              |\n",
        "| buffer_limit  | +INT |The maximum size for your buffer.              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ZJ8yAtPbPw"
      },
      "source": [
        "## frozen lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAvATrJke-sI"
      },
      "outputs": [],
      "source": [
        "game_name =  'FrozenLake-v1'         #⚠️   gym.make(game_name, max_episode_steps=max_steps_for_each_episode, is_slippery=False, map_name=\"4x4\")\n",
        "max_steps_for_each_episode = 20      #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  26                     #⚠️\n",
        "action_size = 4                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 150                   #⚠️\n",
        "history_size  = 0                    #⚠️\n",
        "future_size = 10                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning  = 150\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_r     = 1e-5  \n",
        "greed_epsilon_decay = 0.000          #⚠️               \n",
        "greed_epsilon_min   = 1e-10    \n",
        "beta = 1                     \n",
        "itrtn_for_planning  = 100          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "episode_for_validation = 5\n",
        "\n",
        "batch_size_for_executing = 1         #⚠️\n",
        "\n",
        "batch_size_for_learning = 1          #⚠️       \n",
        "\n",
        "buffer_limit = 10000   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASBb8wJPbPw"
      },
      "source": [
        "## blackjack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WpWlvc8e-sI"
      },
      "outputs": [],
      "source": [
        "game_name = 'Blackjack-v1'           #⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  400                    #⚠️\n",
        "action_size = 2                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 750                   #⚠️\n",
        "history_size  = 0                    #⚠️\n",
        "future_size = 5                      #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning  = 150\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_r     = 1e-5  \n",
        "greed_epsilon_decay = 0.000          #⚠️               \n",
        "greed_epsilon_min   = 1e-10    \n",
        "beta = 1                     \n",
        "itrtn_for_planning  = 100          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "episode_for_validation = 5\n",
        "\n",
        "batch_size_for_executing = 1         #⚠️\n",
        "\n",
        "batch_size_for_learning = 1          #⚠️       \n",
        "\n",
        "buffer_limit = 10000   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgCIHGHWPbPx"
      },
      "source": [
        "## cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_JgJ4E3e-sJ"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'            #⚠️\n",
        "max_steps_for_each_episode = 1000    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  500                    #⚠️\n",
        "action_size = 2                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 750                   #⚠️\n",
        "history_size  = 0                    #⚠️\n",
        "future_size = 25                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning  = 150\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_r     = 1e-5  \n",
        "greed_epsilon_decay = 0.000          #⚠️               \n",
        "greed_epsilon_min   = 1e-10    \n",
        "beta = 1                     \n",
        "itrtn_for_planning  = 100          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "episode_for_validation = 5\n",
        "\n",
        "batch_size_for_executing = 5         #⚠️\n",
        "\n",
        "batch_size_for_learning = 1          #⚠️       \n",
        "\n",
        "buffer_limit = 10000   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MiLL2IAPbPx"
      },
      "source": [
        "## mountain car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcI2SNuje-sJ"
      },
      "outputs": [],
      "source": [
        "game_name =  'MountainCar-v0'        #⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  300                    #⚠️\n",
        "action_size = 3                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 500                   #⚠️\n",
        "history_size  = 0                    #⚠️\n",
        "future_size = 75                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning  = 150\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_r     = 1e-5  \n",
        "greed_epsilon_decay = 0.000          #⚠️               \n",
        "greed_epsilon_min   = 1e-10    \n",
        "beta = 1                     \n",
        "itrtn_for_planning  = 100          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "episode_for_validation = 5\n",
        "\n",
        "batch_size_for_executing = 5         #⚠️\n",
        "\n",
        "batch_size_for_learning = 1          #⚠️       \n",
        "\n",
        "buffer_limit = 10000   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75C-eQ0QPbPy"
      },
      "source": [
        "## acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stJnWGvoe-sK"
      },
      "outputs": [],
      "source": [
        "game_name = 'Acrobot-v1'             #⚠️\n",
        "max_steps_for_each_episode = 250     #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  700                    #⚠️\n",
        "action_size = 3                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 1000                  #⚠️\n",
        "history_size  = 0                    #⚠️\n",
        "future_size = 75                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning  = 150\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_r     = 1e-5  \n",
        "greed_epsilon_decay = 0.000          #⚠️               \n",
        "greed_epsilon_min   = 1e-10    \n",
        "beta = 1                     \n",
        "itrtn_for_planning  = 100          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "episode_for_validation = 5\n",
        "\n",
        "batch_size_for_executing = 5         #⚠️\n",
        "\n",
        "batch_size_for_learning = 1          #⚠️       \n",
        "\n",
        "buffer_limit = 10000   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yi3S50oPbPy"
      },
      "source": [
        "## lunar lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTcH-Fk7e-sK"
      },
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v3\"         #⚠️\n",
        "max_steps_for_each_episode = 250     #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  900                    #⚠️\n",
        "action_size = 4                      #⚠️\n",
        "reward_size = 250                    #⚠️\n",
        "feature_size = 1200                  #⚠️\n",
        "history_size  = 75                   #⚠️\n",
        "future_size = 75                     #⚠️ \n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning  = 150\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_r     = 1e-5  \n",
        "greed_epsilon_decay = 0.000          #⚠️               \n",
        "greed_epsilon_min   = 1e-10    \n",
        "beta = 1                     \n",
        "itrtn_for_planning  = 100          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "episode_for_validation = 5\n",
        "\n",
        "batch_size_for_executing = 5         #⚠️\n",
        "\n",
        "batch_size_for_learning = 1          #⚠️       \n",
        "\n",
        "buffer_limit = 10000   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNIMnHzKk8eV"
      },
      "source": [
        "## door key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYQjUKJue-sK"
      },
      "outputs": [],
      "source": [
        "game_name = \"MiniGrid-DoorKey-5x5-v0\"#⚠️\n",
        "max_steps_for_each_episode = None    #⚠️\n",
        "seed = 1                             #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  257                    #⚠️\n",
        "action_size = 7                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 500                   #⚠️\n",
        "history_size  = 0                    #⚠️\n",
        "future_size = 10                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning  = 150\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_r     = 1e-5  \n",
        "greed_epsilon_decay = 0.000          #⚠️               \n",
        "greed_epsilon_min   = 1e-10    \n",
        "beta = 1                     \n",
        "itrtn_for_planning  = 100          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "episode_for_validation = 5\n",
        "\n",
        "batch_size_for_executing = 1         #⚠️\n",
        "\n",
        "batch_size_for_learning = 1          #⚠️       \n",
        "\n",
        "buffer_limit = 10000   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wyup25fPbPz"
      },
      "source": [
        "## your present config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ounGFZUe-sL"
      },
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v3\"         #⚠️\n",
        "max_steps_for_each_episode = 250     #⚠️\n",
        "seed = None                          #⚠️\n",
        "\n",
        "load_pretrained_model = True\n",
        "\n",
        "ensemble_size = 5                    #◀️\n",
        "\n",
        "state_size =  900                    #⚠️\n",
        "action_size = 4                      #⚠️\n",
        "reward_size = 250                    #⚠️\n",
        "feature_size = 1200                  #⚠️\n",
        "history_size  = 75                   #⚠️\n",
        "future_size = 75                     #⚠️ \n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning  = 150\n",
        "\n",
        "init_ = \"random_uniform\"\n",
        "greed_epsilon_t     = 1\n",
        "greed_epsilon_r     = 1e-5  \n",
        "greed_epsilon_decay = 0.000          #⚠️               \n",
        "greed_epsilon_min   = 1e-10    \n",
        "beta = 1                     \n",
        "itrtn_for_planning  = 100          \n",
        "\n",
        "episode_for_training = 100000\n",
        "\n",
        "episode_for_validation = 5\n",
        "\n",
        "batch_size_for_executing = 5         #⚠️\n",
        "\n",
        "batch_size_for_learning = 1          #⚠️       \n",
        "\n",
        "buffer_limit = 10000   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0p92vcre-sL"
      },
      "outputs": [],
      "source": [
        "episode_for_testing = 100\n",
        "render_for_human = True\n",
        "\n",
        "suffix                 = f\"game_{game_name}-type_{neural_type}-ensemble_{ensemble_size:05d}-learn_{itrtn_for_learning:05d}-plan_{itrtn_for_planning:05d}\"\n",
        "directory              = f'./result/{game_name}/'\n",
        "performance_directory  = f'./result/{game_name}/performace-{suffix}.csv'\n",
        "model_directory        = f'./result/{game_name}/model-{suffix}.pth'\n",
        "buffer_directory       = f'./result/{game_name}/buffer-{suffix}.dill'\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TcwUHixPbPz"
      },
      "source": [
        "# Importing local modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUk1dHbfPbPz"
      },
      "outputs": [],
      "source": [
        "game_modules = {\n",
        "    'FrozenLake-v1': 'envs.env_frozenlake',\n",
        "    'Blackjack-v1': 'envs.env_blackjack',\n",
        "    'CartPole-v1': 'envs.env_cartpole',\n",
        "    'MountainCar-v0': 'envs.env_mountaincar',\n",
        "    'Acrobot-v1': 'envs.env_acrobot',\n",
        "    'LunarLander-v3': 'envs.env_lunarlander',\n",
        "    'MiniGrid-DoorKey-5x5-v0': 'envs.env_doorkey'\n",
        "}\n",
        "if game_name in game_modules:\n",
        "    game_module = __import__(game_modules[game_name], fromlist=['vectorizing_state', 'vectorizing_action', 'vectorizing_reward'])\n",
        "    vectorizing_state  = game_module.vectorizing_state\n",
        "    vectorizing_action = game_module.vectorizing_action\n",
        "    vectorizing_reward = game_module.vectorizing_reward\n",
        "    randomizer         = game_module.randomizer\n",
        "else:\n",
        "    raise RuntimeError('Missing env functions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErBxWJc2PbP0"
      },
      "outputs": [],
      "source": [
        "model_modules = {\n",
        "    'td': 'models.model_td',\n",
        "    'rnn_td': 'models.model_rnn_td',\n",
        "    'gru_td': 'models.model_gru_td',\n",
        "    'rnn_td': 'models.model_rnn_td',\n",
        "    'rnn': 'models.model_rnn',\n",
        "    'gru': 'models.model_rnn',\n",
        "    'lstm': 'models.model_rnn'\n",
        "}\n",
        "if neural_type in model_modules:\n",
        "    model_module = __import__(model_modules[neural_type], fromlist=['build_model'])\n",
        "    build_model  = model_module.build_model\n",
        "else:\n",
        "    raise RuntimeError('Missing model functions')\n",
        "\n",
        "from utils.util_func  import load_performance_from_csv,\\\n",
        "                             load_buffer_from_pickle,\\\n",
        "                             retrieve_history,\\\n",
        "                             retrieve_present,\\\n",
        "                             initialize_future_action, \\\n",
        "                             initialize_desired_reward,\\\n",
        "                             update_future_action, \\\n",
        "                             sequentialize, \\\n",
        "                             update_long_term_experience_replay_buffer,\\\n",
        "                             update_model_list,\\\n",
        "                             limit_buffer,\\\n",
        "                             save_performance_to_csv,\\\n",
        "                             save_buffer_to_pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# planning -> Learning\n",
        "Training mode where your agent will learn offline. You can see here how your agent learn overtime and improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "## Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# creating empty log for recording performance\n",
        "performance_log  = []\n",
        "\n",
        "# setting the last episode number for performance log\n",
        "last_episode = 0\n",
        "\n",
        "# creating model list\n",
        "sequence_size = history_size + future_size \n",
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        action_size,\n",
        "                        reward_size,\n",
        "                        feature_size,\n",
        "                        sequence_size,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        bias,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "# creating space for storing tensors as experience replay buffer\n",
        "history_state_stack        = torch.empty(0).to(device)\n",
        "history_action_stack       = torch.empty(0).to(device)\n",
        "present_state_stack        = torch.empty(0).to(device)\n",
        "future_action_stack        = torch.empty(0).to(device)\n",
        "future_reward_stack        = torch.empty(0).to(device)\n",
        "future_state_stack         = torch.empty(0).to(device)\n",
        "history_state_hash_list    = list()\n",
        "history_action_hash_list   = list()\n",
        "present_state_hash_list    = list()\n",
        "future_action_hash_list    = list()\n",
        "future_reward_hash_list    = list()\n",
        "future_state_hash_list     = list()\n",
        "\n",
        "# load from pre-trained models if needed\n",
        "if load_pretrained_model == True:\n",
        "    try:\n",
        "        model_dict = torch.load(model_directory)\n",
        "        for i, model in enumerate(model_list):\n",
        "            model.load_state_dict(model_dict[f'model_{i}'])\n",
        "        history_state_stack, \\\n",
        "        history_action_stack,\\\n",
        "        present_state_stack, \\\n",
        "        future_action_stack, \\\n",
        "        future_reward_stack, \\\n",
        "        future_state_stack,  \\\n",
        "        history_state_hash_list, \\\n",
        "        history_action_hash_list, \\\n",
        "        present_state_hash_list, \\\n",
        "        future_action_hash_list, \\\n",
        "        future_reward_hash_list, \\\n",
        "        future_state_hash_list = load_buffer_from_pickle(buffer_directory)\n",
        "        history_state_stack    = history_state_stack.to (device) \n",
        "        history_action_stack   = history_action_stack.to(device) \n",
        "        present_state_stack    = present_state_stack.to (device) \n",
        "        future_action_stack    = future_action_stack.to (device) \n",
        "        future_reward_stack    = future_reward_stack.to (device) \n",
        "        future_state_stack     = future_state_stack .to (device) \n",
        "        performance_log        = load_performance_from_csv(performance_directory)\n",
        "        last_episode           = performance_log[-1][0] if len(performance_log) > 0 else 0\n",
        "        greed_epsilon_r        = max(greed_epsilon_r - (greed_epsilon_decay * last_episode), greed_epsilon_min)\n",
        "        print('Loaded pre-trained models.')\n",
        "    except:\n",
        "        print('Failed loading pre-trained models. Now using new models.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "## Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-jpi_m6p3RO",
        "outputId": "41cc7bf4-4028-4a3e-b795-2c52d25bcced"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:51<00:00,  2.92it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.51it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.42it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.35it/s]\n",
            "  0%|          | 2/100000 [38:54<32716:43:02, 1177.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 164\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:03<00:00,  2.37it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.40it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.37it/s]\n",
            "100%|██████████| 150/150 [00:57<00:00,  2.60it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "  0%|          | 3/100000 [1:06:23<38690:34:18, 1392.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 139\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:03<00:00,  2.37it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.55it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "  0%|          | 4/100000 [1:30:32<39308:39:17, 1415.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 5: Summed_Reward = -428.25974835377446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.39it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.58it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.50it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.53it/s]\n",
            "  0%|          | 5/100000 [1:55:15<39987:50:54, 1439.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 116\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:57<00:00,  2.60it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.58it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "  0%|          | 6/100000 [2:15:48<38030:06:14, 1369.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 129\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.56it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.58it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.37it/s]\n",
            "  0%|          | 7/100000 [2:38:22<37891:59:39, 1364.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 102\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:03<00:00,  2.37it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.53it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.38it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.57it/s]\n",
            "  0%|          | 8/100000 [2:57:45<36114:41:13, 1300.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 136\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:58<00:00,  2.54it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.54it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.49it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.53it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "  0%|          | 9/100000 [3:20:59<36934:18:44, 1329.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10: Summed_Reward = -31.933185639124517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:59<00:00,  2.51it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.51it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.40it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.48it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.51it/s]\n",
            "  0%|          | 10/100000 [3:47:04<38945:49:55, 1402.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 119\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:02<00:00,  2.39it/s]\n",
            "100%|██████████| 150/150 [00:57<00:00,  2.59it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.57it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "  0%|          | 11/100000 [4:08:19<37866:04:08, 1363.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 142\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:58<00:00,  2.59it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.52it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.40it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "  0%|          | 12/100000 [4:33:05<38898:19:37, 1400.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 155\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.42it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.48it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "100%|██████████| 150/150 [00:57<00:00,  2.62it/s]\n",
            "  0%|          | 13/100000 [4:59:22<40382:20:56, 1453.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 324\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.58it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.57it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "  0%|          | 14/100000 [5:38:30<47884:46:55, 1724.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 15: Summed_Reward = -478.43427025676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:00<00:00,  2.49it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.49it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.55it/s]\n",
            "  0%|          | 15/100000 [6:04:45<46633:03:01, 1679.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 166\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:55<00:00,  2.68it/s]\n",
            "100%|██████████| 150/150 [00:55<00:00,  2.71it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.58it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.56it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n",
            "  0%|          | 16/100000 [6:31:59<46263:39:09, 1665.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 132\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "100%|██████████| 150/150 [01:04<00:00,  2.31it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.37it/s]\n",
            "  0%|          | 17/100000 [6:55:39<44210:01:38, 1591.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 152\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:05<00:00,  2.30it/s]\n",
            "100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "100%|██████████| 150/150 [01:08<00:00,  2.20it/s]\n",
            "100%|██████████| 150/150 [01:07<00:00,  2.24it/s]\n",
            "100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n",
            "  0%|          | 18/100000 [7:22:25<44321:41:08, 1595.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 153\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:03<00:00,  2.35it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.48it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.52it/s]\n",
            "  0%|          | 19/100000 [7:48:53<44257:52:31, 1593.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20: Summed_Reward = -726.9235884101283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:02<00:00,  2.39it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.37it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.39it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.42it/s]\n",
            "  0%|          | 20/100000 [8:17:14<45153:01:57, 1625.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 192\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [00:57<00:00,  2.61it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.51it/s]\n",
            "  0%|          | 21/100000 [8:49:08<47554:57:35, 1712.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 115\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:59<00:00,  2.51it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.54it/s]\n",
            "  0%|          | 22/100000 [9:10:01<43726:46:46, 1574.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 135\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:59<00:00,  2.53it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.48it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.40it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.54it/s]\n",
            "  0%|          | 23/100000 [9:33:31<42356:57:02, 1525.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 129\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:55<00:00,  2.72it/s]\n",
            "100%|██████████| 150/150 [00:55<00:00,  2.68it/s]\n",
            "100%|██████████| 150/150 [00:56<00:00,  2.63it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.54it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.42it/s]\n",
            "  0%|          | 24/100000 [9:56:05<40930:33:19, 1473.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 25: Summed_Reward = -127.89711955207126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:00<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.34it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.35it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.53it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.54it/s]\n",
            "  0%|          | 25/100000 [10:23:18<42253:44:49, 1521.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 150\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:03<00:00,  2.35it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.40it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.50it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.46it/s]\n",
            "  0%|          | 26/100000 [10:49:00<42426:59:12, 1527.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 135\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:04<00:00,  2.34it/s]\n",
            "100%|██████████| 150/150 [01:06<00:00,  2.25it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n",
            "100%|██████████| 150/150 [01:05<00:00,  2.27it/s]\n",
            "  0%|          | 27/100000 [11:12:42<41542:44:55, 1495.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 155\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:00<00:00,  2.48it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.57it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.52it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "  0%|          | 28/100000 [11:38:49<42134:38:11, 1517.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 138\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:56<00:00,  2.65it/s]\n",
            "100%|██████████| 150/150 [00:57<00:00,  2.62it/s]\n",
            "100%|██████████| 150/150 [00:57<00:00,  2.63it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.55it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.56it/s]\n",
            "  0%|          | 29/100000 [12:02:39<41404:49:23, 1491.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 30: Summed_Reward = -257.78283708309266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:00<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.54it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.38it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.47it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.54it/s]\n",
            "  0%|          | 30/100000 [12:35:19<45314:26:22, 1631.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 97\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:59<00:00,  2.50it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.44it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.50it/s]\n",
            "  0%|          | 31/100000 [12:54:06<41103:49:40, 1480.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 135\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:58<00:00,  2.55it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.50it/s]\n",
            "100%|██████████| 150/150 [00:57<00:00,  2.63it/s]\n",
            "100%|██████████| 150/150 [00:55<00:00,  2.70it/s]\n",
            "  0%|          | 32/100000 [13:17:16<40352:55:04, 1453.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 101\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "100%|██████████| 150/150 [01:04<00:00,  2.34it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.39it/s]\n",
            "100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "  0%|          | 33/100000 [13:36:18<37764:05:38, 1359.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 129\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "100%|██████████| 150/150 [01:07<00:00,  2.21it/s]\n",
            "100%|██████████| 150/150 [01:07<00:00,  2.23it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.35it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.38it/s]\n",
            "  0%|          | 34/100000 [13:59:29<38022:21:34, 1369.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 35: Summed_Reward = -561.5574215314017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:59<00:00,  2.54it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.42it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.58it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.55it/s]\n",
            "  0%|          | 35/100000 [14:25:58<39854:13:41, 1435.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 135\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:58<00:00,  2.58it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.37it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.56it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.46it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "  0%|          | 36/100000 [14:49:20<39575:56:27, 1425.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 136\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:02<00:00,  2.42it/s]\n",
            "100%|██████████| 150/150 [01:00<00:00,  2.48it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.51it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.57it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.38it/s]\n",
            "  0%|          | 37/100000 [15:12:43<39384:52:11, 1418.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 131\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:57<00:00,  2.61it/s]\n",
            "100%|██████████| 150/150 [01:03<00:00,  2.36it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.42it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n",
            "100%|██████████| 150/150 [00:59<00:00,  2.53it/s]\n",
            "  0%|          | 38/100000 [15:35:28<38942:12:41, 1402.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 133\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:01<00:00,  2.43it/s]\n",
            "100%|██████████| 150/150 [01:01<00:00,  2.45it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.40it/s]\n",
            "100%|██████████| 150/150 [01:02<00:00,  2.41it/s]\n",
            "100%|██████████| 150/150 [00:58<00:00,  2.56it/s]\n",
            "  0%|          | 39/100000 [15:59:06<39071:44:47, 1407.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 81\r"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "We don't randomize desired reward anymore because:\n",
        "1 - It is not typical in RL.\n",
        "2 - There are many more effective methods like epsilon-greedy, intrinsic motivation, and reward shaping that can drive an agent to explore effectively.\n",
        "3 - Those methods are designed to balance exploration and exploitation in a way that promotes learning while keeping the agent on a meaningful path toward mastering the environment.\n",
        "\"\"\"\n",
        "\n",
        "# starting each episode\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "    latest_episode = training_episode + last_episode + 1\n",
        "\n",
        "    # initializing summed reward\n",
        "    summed_reward  = 0\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    state_list  = []\n",
        "    action_list = []\n",
        "    reward_list = []\n",
        "    for _ in range(history_size):\n",
        "        state_list .append(torch.zeros(state_size  ).to(device) - 1)\n",
        "        action_list.append(torch.zeros(action_size ).to(device) - 1)\n",
        "        reward_list.append(vectorizing_reward(None, 0, 0, False, reward_size, device)) \n",
        "\n",
        "    # initializing environment\n",
        "    env            = gym.make(game_name, max_episode_steps=max_steps_for_each_episode)\n",
        "    if latest_episode % episode_for_validation != 0:\n",
        "        env = randomizer(env)\n",
        "    state, info    = env.reset(seed = seed)\n",
        "    \n",
        "    # observing state\n",
        "    state          = vectorizing_state(state, False, device)\n",
        "    state_list.append(state)\n",
        "\n",
        "    # starting each step\n",
        "    post_done_counter = 0\n",
        "    post_done_steps = future_size\n",
        "    done_flag = False\n",
        "    done = False\n",
        "    truncated = False\n",
        "    while not done_flag and not truncated:\n",
        "        \n",
        "        \"\"\"\"\n",
        "        We let agent took some history states and actions into consideration.\n",
        "        \"\"\"\n",
        "        # initializing and updating action by desired reward                                  \n",
        "        history_state, \\\n",
        "        history_action  = retrieve_history(state_list, action_list, history_size, device)\n",
        "        present_state   = retrieve_present(state_list, device)\n",
        "        future_action   = initialize_future_action(init_, greed_epsilon_t, greed_epsilon_r, (1, future_size, action_size), device)\n",
        "        desired_reward  = initialize_desired_reward((1, future_size, reward_size), device)\n",
        "        future_action   = update_future_action(itrtn_for_planning,\n",
        "                                               model_list,\n",
        "                                               history_state ,\n",
        "                                               history_action,\n",
        "                                               present_state,\n",
        "                                               future_action,\n",
        "                                               desired_reward,\n",
        "                                               beta)\n",
        "\n",
        "        \"\"\"\n",
        "        We let agent execute several planned actions rather than one at a time to make data gathering more efficient. \n",
        "        batch_size_for_executing shall be less or equal to future_size.\n",
        "        \"\"\"\n",
        "        # taking actions and skip planning \n",
        "        for i in range(batch_size_for_executing):\n",
        "\n",
        "            # observing action\n",
        "            action, action_  = vectorizing_action(future_action[:, i:, :], device)\n",
        "            action_list.append(action)\n",
        "\n",
        "            # executing action\n",
        "            state, reward, done, truncated, info = env.step(action_)\n",
        "\n",
        "            # summing reward\n",
        "            if done:\n",
        "                reward = 0\n",
        "            summed_reward += reward\n",
        "\n",
        "            # observing actual reward\n",
        "            reward = vectorizing_reward(state, reward, summed_reward, done, reward_size, device)\n",
        "            reward_list.append(reward)\n",
        "\n",
        "            # observing state\n",
        "            state = vectorizing_state(state, done, device)\n",
        "            state_list.append(state)\n",
        "\n",
        "            \"\"\"\n",
        "            We expanded the condition for terminating an episode to include the case where the count is smaller than the sum of the history and future sizes. \n",
        "            Though it is contrary to common practice in RL, this is for better handling the sequentialization of the short-term experience replay buffer with fixed window length.\n",
        "            And it is also for agent to plan ahead even after the episode is done.\n",
        "            We give a done flag to state to indicate that the environment is done so that the agent won't be confused. \n",
        "            The done flag shall affect the state in a considerable way to remind the agent that the environment is done.\n",
        "            \"\"\"\n",
        "            # if done then continue for a short period. Then store experience to short term experience replay buffer\n",
        "            if done:\n",
        "                post_done_counter += 1\n",
        "                if post_done_counter >= post_done_steps:\n",
        "                    done_flag = True\n",
        "                    break            \n",
        "            elif truncated:\n",
        "                break\n",
        "            else:\n",
        "                print(f'\\rStep: {len(action_list)+1}\\r', end='', flush=True)\n",
        "                \n",
        "    # closing env\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # recording performance\n",
        "    if latest_episode % episode_for_validation == 0:\n",
        "        print(f'Episode {latest_episode}: Summed_Reward = {summed_reward}')\n",
        "        performance_log.append([latest_episode, summed_reward])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentializing short term experience replay buffer\n",
        "    history_state_list   ,\\\n",
        "    history_action_list   ,\\\n",
        "    present_state_list   ,\\\n",
        "    future_action_list   ,\\\n",
        "    future_reward_list   ,\\\n",
        "    future_state_list    = sequentialize(state_list  ,\n",
        "                                         action_list ,\n",
        "                                         reward_list ,\n",
        "                                         history_size,\n",
        "                                         future_size)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    # storing sequentialized short term experience to long term experience replay buffer \n",
        "    history_state_stack, \\\n",
        "    history_action_stack, \\\n",
        "    present_state_stack, \\\n",
        "    future_action_stack, \\\n",
        "    future_reward_stack, \\\n",
        "    future_state_stack,\\\n",
        "    history_state_hash_list  , \\\n",
        "    history_action_hash_list  , \\\n",
        "    present_state_hash_list  , \\\n",
        "    future_action_hash_list  , \\\n",
        "    future_reward_hash_list  , \\\n",
        "    future_state_hash_list      = update_long_term_experience_replay_buffer(history_state_stack,\n",
        "                                                                            history_action_stack,\n",
        "                                                                            present_state_stack,\n",
        "                                                                            future_action_stack,\n",
        "                                                                            future_reward_stack,\n",
        "                                                                            future_state_stack ,\n",
        "                                                                            history_state_hash_list  ,\n",
        "                                                                            history_action_hash_list  ,\n",
        "                                                                            present_state_hash_list  ,\n",
        "                                                                            future_action_hash_list  ,\n",
        "                                                                            future_reward_hash_list  ,\n",
        "                                                                            future_state_hash_list   ,\n",
        "                                                                            history_state_list   ,\n",
        "                                                                            history_action_list   ,\n",
        "                                                                            present_state_list,\n",
        "                                                                            future_action_list,\n",
        "                                                                            future_reward_list,\n",
        "                                                                            future_state_list )\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \"\"\"\n",
        "    We use batch_size to make training more efficient.\n",
        "    \"\"\"\n",
        "    # training\n",
        "    dataset     = TensorDataset     (history_state_stack,\n",
        "                                     history_action_stack,\n",
        "                                     present_state_stack,\n",
        "                                     future_action_stack,\n",
        "                                     future_reward_stack,\n",
        "                                     future_state_stack  )\n",
        "    model_list  = update_model_list (itrtn_for_learning ,\n",
        "                                     dataset,\n",
        "                                     model_list,\n",
        "                                     batch_size_for_learning\n",
        "                                     )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # limit_buffer\n",
        "    history_state_stack, \\\n",
        "    history_action_stack, \\\n",
        "    present_state_stack, \\\n",
        "    future_action_stack, \\\n",
        "    future_reward_stack, \\\n",
        "    future_state_stack , \\\n",
        "    history_state_hash_list  , \\\n",
        "    history_action_hash_list  , \\\n",
        "    present_state_hash_list  , \\\n",
        "    future_action_hash_list  , \\\n",
        "    future_reward_hash_list  , \\\n",
        "    future_state_hash_list   = limit_buffer(history_state_stack,\n",
        "                                            history_action_stack,\n",
        "                                            present_state_stack,\n",
        "                                            future_action_stack,\n",
        "                                            future_reward_stack,\n",
        "                                            future_state_stack ,\n",
        "                                            history_state_hash_list  ,\n",
        "                                            history_action_hash_list  ,\n",
        "                                            present_state_hash_list  ,\n",
        "                                            future_action_hash_list  ,\n",
        "                                            future_reward_hash_list  ,\n",
        "                                            future_state_hash_list ,\n",
        "                                            buffer_limit  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    We set a decay rate for greed_epsilon_r to make the agent more greedy as time goes by.\n",
        "    We set a lower bound for greed_epsilon_r to prevent it from becoming too small which is similar to initialzing the weights in neural networks to nearly zero.\n",
        "    \"\"\"\n",
        "    # decreasing decay rate\n",
        "    greed_epsilon_r = greed_epsilon_r - greed_epsilon_decay\n",
        "    greed_epsilon_r = max(greed_epsilon_r , greed_epsilon_min)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # saving when reaching episode_for_validation\n",
        "    if latest_episode % episode_for_validation == 0:\n",
        "        \n",
        "        # saving final reward to log\n",
        "        save_performance_to_csv(performance_log, performance_directory)\n",
        "\n",
        "        # saving nn models\n",
        "        model_dict = {}\n",
        "        for i, model in enumerate(model_list):\n",
        "            model_dict[f'model_{i}'] = model.state_dict()\n",
        "        torch.save(model_dict, model_directory)\n",
        "\n",
        "        # saving long term experience replay buffer\n",
        "        save_buffer_to_pickle(buffer_directory,\n",
        "                              history_state_stack,\n",
        "                              history_action_stack,\n",
        "                              present_state_stack,\n",
        "                              future_action_stack,\n",
        "                              future_reward_stack,\n",
        "                              future_state_stack,\n",
        "                              history_state_hash_list,\n",
        "                              history_action_hash_list,\n",
        "                              present_state_hash_list,\n",
        "                              future_action_hash_list,\n",
        "                              future_reward_hash_list,\n",
        "                              future_state_hash_list)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "    # clear up\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yunFuFHxgX"
      },
      "source": [
        "# planning only\n",
        "Testing mode where your trained agent in the training mode will not learn offline. It just keeps running each episode without learning new stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLG0dkigSxeJ"
      },
      "source": [
        "## Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIj0Y-Yf-v_"
      },
      "outputs": [],
      "source": [
        "sequence_size = history_size + future_size \n",
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        action_size,\n",
        "                        reward_size,\n",
        "                        feature_size,\n",
        "                        sequence_size ,\n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        bias,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "model_dict = torch.load(model_directory)\n",
        "for i, model in enumerate(model_list):\n",
        "    model.load_state_dict(model_dict[f'model_{i}'])\n",
        "\n",
        "performance_log        = load_performance_from_csv(performance_directory)\n",
        "last_episode           = performance_log[-1][0] + 1 if len(performance_log) > 0 else 0\n",
        "greed_epsilon_r        = max(greed_epsilon_r - (greed_epsilon_decay * last_episode), greed_epsilon_min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R3maKQXSxeR"
      },
      "source": [
        "## Putting all the previous works into play ... again\n",
        "\n",
        "But this time the agent does not learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nw62kaUbHCb"
      },
      "outputs": [],
      "source": [
        "# score recorder\n",
        "total_summed_reward = 0\n",
        "\n",
        "# starting each episode\n",
        "for testing_episode in range(episode_for_testing):\n",
        "\n",
        "    # initializing summed reward\n",
        "    summed_reward  = 0\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    state_list  = []\n",
        "    action_list = []\n",
        "    for _ in range(history_size):\n",
        "        state_list .append(torch.zeros(state_size  ).to(device) - 1)\n",
        "        action_list.append(torch.zeros(action_size ).to(device) - 1)\n",
        "\n",
        "    # initializing environment\n",
        "    env = gym.make(game_name, max_episode_steps = max_steps_for_each_episode,\n",
        "                   render_mode = \"human\" if render_for_human else None)\n",
        "    env = randomizer(env)\n",
        "    state, info = env.reset(seed = seed)\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "\n",
        "    # observing state\n",
        "    state = vectorizing_state(state, False, device)\n",
        "    state_list.append(state)\n",
        "\n",
        "    # starting each step\n",
        "    done = False\n",
        "    truncated = False\n",
        "    while not done and not truncated:\n",
        "        \n",
        "        # initializing and updating action   \n",
        "        history_state, \\\n",
        "        history_action = retrieve_history(state_list, action_list, history_size, device)\n",
        "        present_state  = retrieve_present(state_list, device)\n",
        "        future_action  = initialize_future_action(init_, greed_epsilon_t, greed_epsilon_r, (1, future_size, action_size), device)\n",
        "        desired_reward = initialize_desired_reward((1, future_size, reward_size), device)\n",
        "        future_action  = update_future_action(itrtn_for_planning,\n",
        "                                              model_list,\n",
        "                                              history_state ,\n",
        "                                              history_action,\n",
        "                                              present_state,\n",
        "                                              future_action,\n",
        "                                              desired_reward,\n",
        "                                              beta)\n",
        "    \n",
        "         # taking actions and skip planning \n",
        "        for i in range(batch_size_for_executing):\n",
        "\n",
        "            print(f'\\rStep: {len(action_list)+1}\\r', end='', flush=True)\n",
        "\n",
        "            # observing action\n",
        "            action, action_  = vectorizing_action(future_action[:, i:, :], device)\n",
        "            action_list.append(action)\n",
        "\n",
        "            # executing action\n",
        "            state, reward, done, truncated, info = env.step(action_)\n",
        "            if render_for_human == True:\n",
        "                env.render()\n",
        "                \n",
        "            # summing reward\n",
        "            summed_reward += reward\n",
        "            \n",
        "            # observing state\n",
        "            state = vectorizing_state(state, done, device)\n",
        "            state_list.append(state)\n",
        "            \n",
        "            # terminating episode if done or truncated\n",
        "            if done or truncated:\n",
        "                break\n",
        "        \n",
        "    # closing env\n",
        "    env.close()\n",
        "\n",
        "    # recording\n",
        "    print(\"Summed reward:\", summed_reward)\n",
        "    print(f'Episode: {testing_episode + 1}')\n",
        "    print('Everaged summed reward:')\n",
        "    total_summed_reward += summed_reward\n",
        "    print(total_summed_reward/(testing_episode + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPyPT-qhrXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
