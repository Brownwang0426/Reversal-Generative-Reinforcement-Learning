{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/Reversal-Generative-Reinforcement-Learning/blob/main-fast/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O00EPp4iLbmV"
      },
      "source": [
        "# Setting up (for local)\n",
        "CUDA Toolkit 11.8 \\\n",
        "cuDNN 8.9.x\\\n",
        "python3.10\\\n",
        "git clone --branch main https://github.com/Brownwang0426/Reversal-Generative-Reinforcement-Learning.git\\\n",
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT4dy82Rf-v1"
      },
      "source": [
        "# Setting up (for colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4khPQ2_Kf-v1"
      },
      "outputs": [],
      "source": [
        "# before restart\n",
        "!sudo apt-get install python3.10\n",
        "!git clone --branch main-fast https://github.com/Brownwang0426/Reversal-Generative-Reinforcement-Learning.git\n",
        "import os\n",
        "os.chdir('/content/Reversal-Generative-Reinforcement-Learning')\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW2IWlRPLbmh"
      },
      "outputs": [],
      "source": [
        "# restart\n",
        "from IPython.display import display, Javascript\n",
        "display(Javascript('google.colab.kernel.restart()'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31qitDd7Lbmi"
      },
      "outputs": [],
      "source": [
        "# after restart\n",
        "import os\n",
        "os.chdir('/content/Reversal-Generative-Reinforcement-Learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import TimeLimit\n",
        "import minigrid\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n",
        "\n",
        "import dill\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import concurrent.futures\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj5V_vlwSxd8"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    device_ = torch.device(\"cpu\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    device_ = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKNx6iUnXQy"
      },
      "source": [
        "# Control board\n",
        "\n",
        "Crucial configurations regarding how your agent will learn in the environment. The meanings are as follow:\n",
        "(the configs starting with ⚠️ are what we suggest you must tune according to your specific need in your task)\n",
        "(the configs starting with ◀️ are what we suggest you to play with to see the effect)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7r-W0IeGBR0"
      },
      "source": [
        "\n",
        "## Configs meaning\n",
        "| Configs   | Type   | Description                                                                 |\n",
        "|------------|--------|-----------------------------------------------------------------------------|\n",
        "| ⚠️game_name  | STR| The name of the environment.                                |\n",
        "| ⚠️max_steps_for_each_episode | +INT | The maximun steps that the agent will go through while not done. In some environments, it is crucial to increase your \"max_steps_for_each_episode\" so that your agent can \"live long enough\" to obatin some better rewards to gradually and heuristically learn better strategy.                    |\n",
        "| ⚠️seed | +INT/None | The seed for environment. None for random environment each episode.                    |\n",
        "| load_pretrained_model  | BOLEAN |Whether you want to load previous trained model.                          |\n",
        "| ◀️ensemble_size  | +INT | The size of the neural ensemble which the agent is comprised of. The bigger, the better, but the longer training time without parallel training. :-D                  |\n",
        "| ⚠️validation_size    | +INT | delayed learning interval           |\n",
        "| ⚠️state_size  | +INT | The size of the state as input data.                    |\n",
        "| ⚠️action_size   | +INT | The size of action per step as input data.   |\n",
        "| ⚠️reward_size  | +INT |The size of the reward as output data.                          |\n",
        "| ⚠️feature_size   | +INT |The size of the hidden layers. **`Shall be bigger than the sum of state_size, action_size and reward_size`**.      |\n",
        "| ⚠️history_size  | 0/+INT |How many steps in the history for state and action will the agent take into consideration.                           |\n",
        "| ⚠️future_size  | +INT |The length of the sequence of actions in learning phase. Namely, how many steps in the future the agent will predict or use to discern the present best action.                |\n",
        "| ⚠️neural_type  | STR |  [**`rnn`**, **`gru`**, **`lstm`**, **`td`**] The type of neural network you prefer. For now, we support rnn, gru, lstm, and td (Transformer decoder only). More to come in the future (or you can build one yourself :-D in the models repository).           |\n",
        "| ⚠️num_layers  | +INT |The number of layers in rnn, gru, lstm, and td (Transformer decoder only).     |\n",
        "| ⚠️num_heads  | +INT/None |The number of heads in multi-head attention. **`Shall be able to devide feature_size`**. **`Shall be None for non-attention neural_type`**.                         |\n",
        "| init   | STR | [**`random_normal`**, **`random_uniform`**, **`xavier_normal`**, **`xavier_uniform`**, **`glorot_normal`**, **`glorot_uniform`**] The initialization method you prefer for initiating neural net ensemble of your agent.                          |\n",
        "| opti   | STR | [**`adam`**, **`sgd`**, **`rmsprop`**]  The optimization method you prefer.             |\n",
        "| loss  | STR | [**`mean_squared_error`**, **`binary_crossentropy`**] The loss or error function you prefer.                           |\n",
        "| bias  | BOLEAN |Whether you want add bias.                          |\n",
        "| drop_rate   | 0/+FLOAT |The drop-rate for drop-out.              |\n",
        "| alpha   | 0/+FLOAT |The learning rate for neural networks weight matrices.                           |\n",
        "| itrtn_for_learning   | +INT |The iteration for learning per experience.              |\n",
        "| beta  |  0/+FLOAT |The updating rate for updating actions of the agent.              |\n",
        "| max_itrtn_for_planning   | +INT |The maximum iteration for planning before scaled down by average reward.              |\n",
        "| window_size   | +INT | The window size to determine the latest averaged reward in the replay buffer.          |\n",
        "| itrtn_for_planning (adaptive)  |  +INT |The iteration for updating actions of the agent.                           |\n",
        "| episode_for_training  | +INT |How many epsiodes will your agent run in the training mode where your agent will learn offline.              |\n",
        "| buffer_limit  | +INT |The maximum size for your buffer.              |\n",
        "| per  | BOLEAN | Whether to use Prioritized Experience Replay or not.              |\n",
        "| render_for_human  | BOLEAN | If you want to visualize the process.              |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ZJ8yAtPbPw"
      },
      "source": [
        "## frozen lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAvATrJke-sI"
      },
      "outputs": [],
      "source": [
        "game_name =  'FrozenLake-v1'         #⚠️   gym.make(game_name, max_episode_steps=max_steps_for_each_episode, is_slippery=False, map_name=\"4x4\")\n",
        "max_steps_for_each_episode = 25      #⚠️\n",
        "seed = None                          #⚠️\n",
        "load_pretrained_model = True\n",
        "ensemble_size = 5                    #◀️\n",
        "validation_size = 10                 #⚠️\n",
        "state_size = 36                      #⚠️\n",
        "action_size = 4                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 100                   #⚠️\n",
        "history_size = 25                    #⚠️\n",
        "future_size = 25                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgCIHGHWPbPx"
      },
      "source": [
        "## cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_JgJ4E3e-sJ"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'            #⚠️\n",
        "max_steps_for_each_episode = 1000    #⚠️\n",
        "seed = None                          #⚠️\n",
        "load_pretrained_model = True\n",
        "ensemble_size = 5                    #◀️\n",
        "validation_size = 10                 #⚠️\n",
        "state_size =  260                    #⚠️\n",
        "action_size = 2                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 400                   #⚠️\n",
        "history_size = 1000                  #⚠️\n",
        "future_size = 50                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MiLL2IAPbPx"
      },
      "source": [
        "## mountain car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcI2SNuje-sJ"
      },
      "outputs": [],
      "source": [
        "game_name =  'MountainCar-v0'        #⚠️\n",
        "max_steps_for_each_episode = 250     #⚠️\n",
        "seed = None                          #⚠️\n",
        "load_pretrained_model = True\n",
        "ensemble_size = 5                    #◀️\n",
        "validation_size = 10                 #⚠️\n",
        "state_size =  160                    #⚠️\n",
        "action_size = 3                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 300                   #⚠️\n",
        "history_size  = 250                  #⚠️\n",
        "future_size = 150                    #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75C-eQ0QPbPy"
      },
      "source": [
        "## acrobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stJnWGvoe-sK"
      },
      "outputs": [],
      "source": [
        "game_name = 'Acrobot-v1'             #⚠️\n",
        "max_steps_for_each_episode = 250     #⚠️\n",
        "seed = None                          #⚠️\n",
        "load_pretrained_model = True\n",
        "ensemble_size = 5                    #◀️\n",
        "validation_size = 10                 #⚠️\n",
        "state_size =  360                    #⚠️\n",
        "action_size = 3                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 500                   #⚠️\n",
        "history_size  = 250                  #⚠️\n",
        "future_size = 150                    #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yi3S50oPbPy"
      },
      "source": [
        "## lunar lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTcH-Fk7e-sK"
      },
      "outputs": [],
      "source": [
        "game_name = \"LunarLander-v3\"         #⚠️\n",
        "max_steps_for_each_episode = 200     #⚠️\n",
        "seed = None                          #⚠️\n",
        "load_pretrained_model = True\n",
        "ensemble_size = 10                   #◀️\n",
        "validation_size = 10                 #⚠️\n",
        "state_size =  460                    #⚠️\n",
        "action_size = 4                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 500                   #⚠️\n",
        "history_size = 200                   #⚠️\n",
        "future_size = 150                    #⚠️ \n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNIMnHzKk8eV"
      },
      "source": [
        "## door key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYQjUKJue-sK"
      },
      "outputs": [],
      "source": [
        "game_name = \"MiniGrid-DoorKey-5x5-v0\"#⚠️\n",
        "max_steps_for_each_episode = 25      #⚠️\n",
        "seed = 1                             #⚠️\n",
        "load_pretrained_model = True\n",
        "ensemble_size = 5                    #◀️\n",
        "validation_size = 10                 #⚠️\n",
        "state_size =  267                    #⚠️\n",
        "action_size = 6                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 400                   #⚠️\n",
        "history_size  = 25                   #⚠️\n",
        "future_size = 25                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wyup25fPbPz"
      },
      "source": [
        "## your present config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ounGFZUe-sL"
      },
      "outputs": [],
      "source": [
        "game_name = 'CartPole-v1'            #⚠️\n",
        "max_steps_for_each_episode = 1000    #⚠️\n",
        "seed = None                          #⚠️\n",
        "load_pretrained_model = True\n",
        "ensemble_size = 5                    #◀️\n",
        "validation_size = 10                 #⚠️\n",
        "state_size =  260                    #⚠️\n",
        "action_size = 2                      #⚠️\n",
        "reward_size = 100                    #⚠️\n",
        "feature_size = 400                   #⚠️\n",
        "history_size = 1000                  #⚠️\n",
        "future_size = 50                     #⚠️\n",
        "neural_type = 'td'                   #⚠️\n",
        "num_layers = 3                       #⚠️\n",
        "num_heads = 10                       #⚠️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtNik5N6Lbmt"
      },
      "outputs": [],
      "source": [
        "init = \"xavier_normal\"\n",
        "opti = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.1                  \n",
        "itrtn_for_learning = 1500\n",
        "beta = 0.1     \n",
        "max_itrtn_for_planning = 50         \n",
        "window_size = 50\n",
        "episode_for_training = 100000   \n",
        "buffer_limit = 50000   \n",
        "per = False\n",
        "render_for_human = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0p92vcre-sL"
      },
      "outputs": [],
      "source": [
        "\n",
        "suffix                 = f\"game_{game_name}-type_{neural_type}-ensemble_{ensemble_size:05d}-learn_{itrtn_for_learning:05d}\"\n",
        "directory              = f'./result/{game_name}/'\n",
        "performance_directory  = f'./result/{game_name}/performace-{suffix}.csv'\n",
        "model_directory        = f'./result/{game_name}/model-{suffix}.pth'\n",
        "buffer_directory       = f'./result/{game_name}/buffer-{suffix}.dill'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TcwUHixPbPz"
      },
      "source": [
        "# Importing local modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUk1dHbfPbPz"
      },
      "outputs": [],
      "source": [
        "game_modules = {\n",
        "    'FrozenLake-v1': 'envs.env_frozenlake',\n",
        "    'CartPole-v1': 'envs.env_cartpole',\n",
        "    'MountainCar-v0': 'envs.env_mountaincar',\n",
        "    'Acrobot-v1': 'envs.env_acrobot',\n",
        "    'LunarLander-v3': 'envs.env_lunarlander',\n",
        "    'MiniGrid-DoorKey-5x5-v0': 'envs.env_doorkey'\n",
        "}\n",
        "if game_name in game_modules:\n",
        "    game_module = __import__(game_modules[game_name], fromlist=['vectorizing_state', 'vectorizing_action', 'vectorizing_reward', 'averaging_reward', 'randomizer'])\n",
        "    vectorizing_state   = game_module.vectorizing_state\n",
        "    vectorizing_action  = game_module.vectorizing_action\n",
        "    vectorizing_reward  = game_module.vectorizing_reward\n",
        "    averaging_reward    = game_module.averaging_reward\n",
        "    randomizer          = game_module.randomizer\n",
        "else:\n",
        "    raise RuntimeError('Missing env functions')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErBxWJc2PbP0"
      },
      "outputs": [],
      "source": [
        "model_modules = {\n",
        "    'td': 'models.model_td',\n",
        "    'rnn': 'models.model_rnn',\n",
        "    'gru': 'models.model_rnn',\n",
        "    'lstm': 'models.model_rnn'\n",
        "}\n",
        "if neural_type in model_modules:\n",
        "    model_module = __import__(model_modules[neural_type], fromlist=['build_model'])\n",
        "    build_model  = model_module.build_model\n",
        "else:\n",
        "    raise RuntimeError('Missing model functions')\n",
        "\n",
        "from utils.util_func  import load_performance_from_csv,\\\n",
        "                             load_buffer_from_pickle,\\\n",
        "                             retrieve_history,\\\n",
        "                             retrieve_present,\\\n",
        "                             initialize_future_action, \\\n",
        "                             initialize_desired_reward,\\\n",
        "                             update_future_action, \\\n",
        "                             sequentialize, \\\n",
        "                             update_long_term_experience_replay_buffer,\\\n",
        "                             update_model_list,\\\n",
        "                             limit_buffer,\\\n",
        "                             save_performance_to_csv,\\\n",
        "                             save_buffer_to_pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iigp5dSf-v5"
      },
      "source": [
        "# planning -> Learning\n",
        "Training mode where your agent will learn offline. You can see here how your agent learn overtime and improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6aXOy7SxeE"
      },
      "source": [
        "## Creating or loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsXAP3sNf-v8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# creating empty log for recording performance\n",
        "performance_log  = []\n",
        "\n",
        "# setting the last episode number for performance log\n",
        "last_episode = 0\n",
        "\n",
        "# creating model list\n",
        "model_list = []\n",
        "for _ in range(ensemble_size):\n",
        "    model = build_model(state_size,\n",
        "                        action_size,\n",
        "                        reward_size,\n",
        "                        feature_size,\n",
        "                        history_size,\n",
        "                        future_size, \n",
        "                        neural_type,\n",
        "                        num_layers,\n",
        "                        num_heads,\n",
        "                        init,\n",
        "                        opti,\n",
        "                        loss,\n",
        "                        bias,\n",
        "                        drop_rate,\n",
        "                        alpha)\n",
        "    model.to(device)\n",
        "    model_list.append(model)\n",
        "\n",
        "# creating space for storing tensors as experience replay buffer\n",
        "history_state_stack    = torch.empty(0).to(device_, non_blocking=True)\n",
        "present_state_stack    = torch.empty(0).to(device_, non_blocking=True)\n",
        "future_action_stack    = torch.empty(0).to(device_, non_blocking=True)\n",
        "future_reward_stack    = torch.empty(0).to(device_, non_blocking=True)\n",
        "history_state_hash_set = set()\n",
        "present_state_hash_set = set()\n",
        "future_action_hash_set = set()\n",
        "future_reward_hash_set = set()\n",
        "\n",
        "# load from pre-trained models if needed\n",
        "if load_pretrained_model == True:\n",
        "    try:\n",
        "        model_dict = torch.load(model_directory)\n",
        "        for i, model in enumerate(model_list):\n",
        "            model.load_state_dict(model_dict[f'model_{i}'])\n",
        "        history_state_stack, \\\n",
        "        present_state_stack, \\\n",
        "        future_action_stack, \\\n",
        "        future_reward_stack, \\\n",
        "        history_state_hash_set, \\\n",
        "        present_state_hash_set, \\\n",
        "        future_action_hash_set, \\\n",
        "        future_reward_hash_set = load_buffer_from_pickle(buffer_directory)\n",
        "        history_state_stack    = history_state_stack.to (device_) \n",
        "        present_state_stack    = present_state_stack.to (device_) \n",
        "        future_action_stack    = future_action_stack.to (device_) \n",
        "        future_reward_stack    = future_reward_stack.to (device_) \n",
        "        performance_log        = load_performance_from_csv(performance_directory)\n",
        "        last_episode           = performance_log[-1][0] if len(performance_log) > 0 else 0\n",
        "        print('Loaded pre-trained models.')\n",
        "    except:\n",
        "        print('Failed loading pre-trained models. Now using new models.')\n",
        "\n",
        "# retreive highest reward\n",
        "if len(performance_log) > 0:\n",
        "    itrtn_for_planning = averaging_reward([entry[1] for entry in performance_log], max_itrtn_for_planning, window_size)\n",
        "else:\n",
        "    itrtn_for_planning = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInxZXYjSxeI"
      },
      "source": [
        "## Putting all the previous works into play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-jpi_m6p3RO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# starting each episode\n",
        "for training_episode in tqdm(range(episode_for_training)):\n",
        "    current_episode  = training_episode + last_episode + 1\n",
        "\n",
        "    # initializing summed reward\n",
        "    summed_reward  = 0\n",
        "\n",
        "    # initializing short term experience replay buffer\n",
        "    state_list  = []\n",
        "    action_list = []\n",
        "    reward_list = []\n",
        "    for _ in range(history_size):\n",
        "        state_list .append(torch.zeros(state_size  ).to(device_, non_blocking=True) - 1 )\n",
        "        action_list.append(torch.zeros(action_size ).to(device_, non_blocking=True) - 1 )\n",
        "        reward_list.append(torch.zeros(reward_size ).to(device_, non_blocking=True) - 1 )\n",
        "\n",
        "    # initializing environment\n",
        "    if game_name == 'FrozenLake-v1'  :\n",
        "        env        = gym.make(game_name, max_episode_steps=max_steps_for_each_episode, is_slippery=False, map_name=\"4x4\", render_mode = \"human\" if render_for_human else None)\n",
        "    else:\n",
        "        env        = gym.make(game_name, max_episode_steps=max_steps_for_each_episode, render_mode = \"human\" if render_for_human else None)\n",
        "    state, info    = env.reset(seed = seed)\n",
        "    if render_for_human == True:\n",
        "        env.render()\n",
        "\n",
        "    # observing state\n",
        "    state          = vectorizing_state(state, False, False, device_)\n",
        "    state_list.append(state)\n",
        "\n",
        "    # starting each step\n",
        "    post_done_truncated_counter = 0\n",
        "    post_done_truncated_steps = future_size\n",
        "    done_truncated_flag = False\n",
        "    total_step = 0\n",
        "    while not done_truncated_flag:\n",
        "\n",
        "        \"\"\"\n",
        "        We let agent took some history states into consideration.\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        The final desired reward is factually the last time step in desired reward.\n",
        "        \"\"\"\n",
        "        # initializing and updating action by desired reward\n",
        "        history_state   = retrieve_history(state_list, action_list, history_size, device_)\n",
        "        present_state   = retrieve_present(state_list, device_)\n",
        "        future_action   = initialize_future_action ((1, future_size, action_size), device_)\n",
        "        desired_reward  = initialize_desired_reward((1, future_size, reward_size), device_)\n",
        "        future_action   = update_future_action(1 + itrtn_for_planning ,\n",
        "                                               model_list,\n",
        "                                               history_state,\n",
        "                                               present_state,\n",
        "                                               future_action,\n",
        "                                               desired_reward,\n",
        "                                               beta)\n",
        "\n",
        "        # observing action\n",
        "        action, action_  = vectorizing_action(future_action, device_)\n",
        "        action_list.append(action)\n",
        "\n",
        "        # executing action\n",
        "        state, reward, done, truncated, info = env.step(action_)\n",
        "        if (render_for_human == True) and (post_done_truncated_counter == 0):\n",
        "            env.render()\n",
        "\n",
        "        # summing reward\n",
        "        if post_done_truncated_counter > 0:\n",
        "            reward = 0\n",
        "        summed_reward += reward\n",
        "\n",
        "        # observing actual reward\n",
        "        reward = vectorizing_reward(state, done, truncated, reward, summed_reward, reward_size, device_)\n",
        "        reward_list.append(reward)\n",
        "\n",
        "        # observing state\n",
        "        state = vectorizing_state(state, done, truncated, device_)\n",
        "        state_list.append(state)\n",
        "\n",
        "        \"\"\"\n",
        "        We expanded the condition for terminating an episode to include the case where the count is smaller than the sum of the history and future sizes.\n",
        "        Though it is contrary to common practice in RL, this is for better handling the sequentialization of the short-term experience replay buffer with fixed window length.\n",
        "        And it is also for agent to plan ahead even after the episode is done.\n",
        "        We give a done flag to state to indicate that the environment is done so that the agent won't be confused.\n",
        "        The done flag shall affect the state in a considerable way to remind the agent that the environment is done.\n",
        "        \"\"\"\n",
        "        # if done then continue for a short period. Then store experience to short term experience replay buffer\n",
        "        if done or truncated:\n",
        "            post_done_truncated_counter += 1\n",
        "            if post_done_truncated_counter >= post_done_truncated_steps:\n",
        "                done_truncated_flag = True\n",
        "                break\n",
        "        else:\n",
        "            total_step += 1\n",
        "            print(f'\\rStep: {total_step}\\r', end='', flush=True)\n",
        "\n",
        "    # closing env\n",
        "    env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # recording performance\n",
        "    print(f'Episode {current_episode}: Summed_Reward = {summed_reward}')\n",
        "    performance_log.append([current_episode, summed_reward])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # sequentializing short term experience replay buffer\n",
        "    history_state_list   ,\\\n",
        "    present_state_list   ,\\\n",
        "    future_action_list   ,\\\n",
        "    future_reward_list    = sequentialize(state_list  ,\n",
        "                                          action_list ,\n",
        "                                          reward_list ,\n",
        "                                          history_size,\n",
        "                                          future_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    We dropped duplicated experiences in the buffer.\n",
        "    \"\"\"\n",
        "    # storing sequentialized short term experience to long term experience replay buffer\n",
        "    history_state_stack, \\\n",
        "    present_state_stack, \\\n",
        "    future_action_stack, \\\n",
        "    future_reward_stack, \\\n",
        "    history_state_hash_set  , \\\n",
        "    present_state_hash_set  , \\\n",
        "    future_action_hash_set  , \\\n",
        "    future_reward_hash_set     = update_long_term_experience_replay_buffer(history_state_stack,\n",
        "                                                                           present_state_stack,\n",
        "                                                                           future_action_stack,\n",
        "                                                                           future_reward_stack,\n",
        "                                                                           history_state_hash_set  ,\n",
        "                                                                           present_state_hash_set  ,\n",
        "                                                                           future_action_hash_set  ,\n",
        "                                                                           future_reward_hash_set  ,\n",
        "                                                                           history_state_list   ,\n",
        "                                                                           present_state_list,\n",
        "                                                                           future_action_list,\n",
        "                                                                           future_reward_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # training\n",
        "    if current_episode % validation_size == 0:\n",
        "        dataset     = TensorDataset    (history_state_stack,\n",
        "                                        present_state_stack,\n",
        "                                        future_action_stack,\n",
        "                                        future_reward_stack)\n",
        "        model_list  = update_model_list(itrtn_for_learning ,\n",
        "                                        dataset,\n",
        "                                        model_list,\n",
        "                                        per\n",
        "                                        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        We limit buffer to save vram.\n",
        "        \"\"\"\n",
        "        # limit_buffer\n",
        "        history_state_stack, \\\n",
        "        present_state_stack, \\\n",
        "        future_action_stack, \\\n",
        "        future_reward_stack, \\\n",
        "        history_state_hash_set  , \\\n",
        "        present_state_hash_set  , \\\n",
        "        future_action_hash_set  , \\\n",
        "        future_reward_hash_set  = limit_buffer(history_state_stack,\n",
        "                                               present_state_stack,\n",
        "                                               future_action_stack,\n",
        "                                               future_reward_stack,\n",
        "                                               history_state_hash_set  ,\n",
        "                                               present_state_hash_set  ,\n",
        "                                               future_action_hash_set  ,\n",
        "                                               future_reward_hash_set  ,\n",
        "                                               buffer_limit  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # saving nn models\n",
        "        model_dict = {}\n",
        "        for i, model in enumerate(model_list):\n",
        "            model_dict[f'model_{i}'] = model.state_dict()\n",
        "        torch.save(model_dict, model_directory)\n",
        "\n",
        "        # saving long term experience replay buffer\n",
        "        save_buffer_to_pickle(buffer_directory,\n",
        "                              history_state_stack,\n",
        "                              present_state_stack,\n",
        "                              future_action_stack,\n",
        "                              future_reward_stack,\n",
        "                              history_state_hash_set,\n",
        "                              present_state_hash_set,\n",
        "                              future_action_hash_set,\n",
        "                              future_reward_hash_set)\n",
        "\n",
        "        # saving final reward to log\n",
        "        save_performance_to_csv(performance_log, performance_directory)\n",
        "\n",
        "        # retreive highest reward\n",
        "        itrtn_for_planning = averaging_reward([entry[1] for entry in performance_log], max_itrtn_for_planning, window_size)\n",
        "\n",
        "        # clear up\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRqcWpN1Lbmx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Il8t3QZSqDn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS8hE4VuLbmx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
